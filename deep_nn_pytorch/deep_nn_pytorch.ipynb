{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4e959aab",
      "metadata": {
        "id": "4e959aab"
      },
      "source": [
        "# Deep Neural Network com Pytorch\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Semana 1\n",
        "Tensores 1D, Tensores Bidimensionais, Derivativos no Pytorch, Conjunto de Dados Simples, Conjunto de Dados\n",
        "\n",
        "## Semana 2\n",
        "Regressão Linear em 1D, Treinamento de Regressão Linear, Gradiente Descendente e Custo, Pytorch Slope, Gradiente Descendente Estocástico, Dataloader, Mini-Batch Gradient Descent, Treinamento, Validação e Divisão de Dados\n",
        "\n",
        "## Semana 3\n",
        "Regressão Linear Múltipla, Regressão Linear de Múltiplas Saídas, Regressão Logística para Classificação\n",
        "\n",
        "## Semana 4\n",
        "Previsão Softmax, Função Softmax, Softmax Pytorch, Redes Neurais Rasas\n",
        "\n",
        "## Semana 5\n",
        "Redes Neurais Profundas, Desistência, Pesos e Inicialização da Rede Neural, Gradient Descent com Momentum, Normalização em Lote\n",
        "\n",
        "## Semana 6\n",
        "Convolução, Funções de Ativação e Max Polling, Vários Canais de Entrada e Saída, Rede Neural Convolucional, Modelos de Visão de Lanterna"
      ],
      "metadata": {
        "id": "0ekD9EwwhUNp"
      },
      "id": "0ekD9EwwhUNp"
    },
    {
      "cell_type": "markdown",
      "id": "d5b75b48",
      "metadata": {
        "id": "d5b75b48"
      },
      "source": [
        "## Semana 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3896b479",
      "metadata": {
        "id": "3896b479"
      },
      "source": [
        "### Tensores 1D"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Em PyTorch, tensores são estruturas fundamentais que representam dados multi-dimensionais, semelhantes a arrays ou matrizes em outras linguagens. Eles são a base para a construção e manipulação de modelos de aprendizado de máquina e redes neurais profundas. Aqui estão alguns pontos importantes sobre tensores em PyTorch:\n",
        "\n",
        "1. **Similaridade com NumPy:** PyTorch tensores são semelhantes aos arrays do NumPy e, muitas vezes, podem ser convertidos de um para o outro. Isso facilita a integração com bibliotecas científicas em Python.\n",
        "\n",
        "2. **Suporte a GPU:** PyTorch permite a computação em GPUs para acelerar operações. Os tensores podem ser movidos para uma GPU para aproveitar o poder de processamento paralelo.\n",
        "\n",
        "3. **Operações Matemáticas:** PyTorch fornece uma ampla variedade de operações matemáticas que podem ser aplicadas a tensores. Isso inclui operações aritméticas, funções trigonométricas, álgebra linear, entre outras.\n",
        "\n",
        "4. **Autograd:** Uma característica fundamental é o sistema de autograd, que automaticamente calcula gradientes para tensores. Isso é crucial para a otimização de modelos de aprendizado de máquina.\n",
        "\n",
        "5. **Criação de Tensores:** Você pode criar tensores de várias maneiras, seja inicializando-os com valores específicos, gerando números aleatórios, ou a partir de dados existentes.\n",
        "\n",
        "Exemplo de criação de um tensor em PyTorch:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "# Criando um tensor com valores específicos\n",
        "tensor_exemplo = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "\n",
        "print(tensor_exemplo)\n",
        "```\n",
        "\n",
        "Este é um conceito básico, e há muito mais para explorar ao trabalhar com tensores em PyTorch, especialmente ao construir e treinar redes neurais profundas. Se tiver mais perguntas ou se quiser abordar aspectos específicos, sinta-se à vontade para perguntar!"
      ],
      "metadata": {
        "id": "tOBB2-AOYAXP"
      },
      "id": "tOBB2-AOYAXP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensores 1D em PyTorch são estruturas de dados unidimensionais que podem armazenar uma sequência de elementos. Esses tensores são semelhantes a vetores ou listas unidimensionais em outras linguagens de programação. Eles são úteis para representar dados ao longo de uma única dimensão, como séries temporais, uma linha de pixels de uma imagem ou um conjunto de valores.\n",
        "\n",
        "Aqui está um exemplo de como criar e trabalhar com um tensor 1D em PyTorch:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "# Criando um tensor 1D\n",
        "tensor_1d = torch.tensor([1, 2, 3, 4, 5])\n",
        "\n",
        "# Acessando elementos do tensor\n",
        "print(tensor_1d[0])  # Saída: 1\n",
        "print(tensor_1d[2])  # Saída: 3\n",
        "\n",
        "# Operações matemáticas em tensores 1D\n",
        "tensor_resultado = tensor_1d * 2\n",
        "print(tensor_resultado)  # Saída: tensor([2, 4, 6, 8, 10])\n",
        "```\n",
        "\n",
        "Neste exemplo, `tensor_1d` é um tensor 1D contendo os valores de 1 a 5. Você pode acessar elementos individualmente e realizar operações matemáticas diretamente nos tensores.\n",
        "\n",
        "Os tensores 1D são frequentemente usados em problemas onde os dados estão organizados de forma linear, e são a base para a construção de estruturas de dados mais complexas, como matrizes bidimensionais (tensores 2D) e tensores de ordens superiores."
      ],
      "metadata": {
        "id": "nNz_UKZWZnXW"
      },
      "id": "nNz_UKZWZnXW"
    },
    {
      "cell_type": "markdown",
      "id": "1099db48",
      "metadata": {
        "id": "1099db48"
      },
      "source": [
        "### Tensores Bidimensionais"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensores bidimensionais em PyTorch são estruturas de dados que representam matrizes, ou seja, arranjos retangulares de elementos organizados em duas dimensões. Eles são comumente utilizados para representar dados tabulares, imagens, ou qualquer outra informação que possa ser organizada em linhas e colunas.\n",
        "\n",
        "Aqui está um exemplo básico de criação e manipulação de um tensor bidimensional em PyTorch:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "# Criando um tensor bidimensional (matriz)\n",
        "tensor_2d = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "\n",
        "# Acessando elementos do tensor bidimensional\n",
        "print(tensor_2d[0, 1])  # Saída: 2 (linha 0, coluna 1)\n",
        "\n",
        "# Operações matemáticas em tensores bidimensionais\n",
        "tensor_resultado = tensor_2d * 2\n",
        "print(tensor_resultado)\n",
        "# Saída:\n",
        "# tensor([[ 2,  4,  6],\n",
        "#         [ 8, 10, 12],\n",
        "#         [14, 16, 18]])\n",
        "```\n",
        "\n",
        "Neste exemplo, `tensor_2d` é uma matriz 3x3 com elementos de 1 a 9. Você pode acessar elementos individualmente usando índices de linha e coluna, e realizar operações matemáticas diretamente nos tensores bidimensionais.\n",
        "\n",
        "Tensores bidimensionais são frequentemente usados em tarefas de processamento de imagens, onde cada elemento da matriz pode representar um pixel. Eles também são essenciais para a construção e treinamento de modelos de aprendizado de máquina, especialmente redes neurais profundas, onde as entradas muitas vezes são representadas por tensores bidimensionais."
      ],
      "metadata": {
        "id": "e70I0U5jlGLT"
      },
      "id": "e70I0U5jlGLT"
    },
    {
      "cell_type": "code",
      "source": [
        "## Esses são apenas alguns exemplos.\n",
        "# Criando dois tensores bidimensionais\n",
        "import torch\n",
        "tensor_a = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "tensor_b = torch.tensor([[7, 8, 9], [10, 11, 12]])\n",
        "\n",
        "# Soma de tensores\n",
        "soma = tensor_a + tensor_b\n",
        "print(\"Soma:\")\n",
        "print(soma)\n",
        "\n",
        "# Subtração de tensores\n",
        "subtracao = tensor_a - tensor_b\n",
        "print(\"\\nSubtração:\")\n",
        "print(subtracao)\n",
        "\n",
        "# Multiplicação de tensores (element-wise)\n",
        "multiplicacao_elementwise = tensor_a * tensor_b\n",
        "print(\"\\nMultiplicação (element-wise):\")\n",
        "print(multiplicacao_elementwise)\n",
        "\n",
        "# Multiplicação de tensores (produto matricial)\n",
        "produto_matricial = torch.matmul(tensor_a, tensor_b.T)  # Transpondo tensor_b para alinhar dimensões*\n",
        "print(\"\\nProduto Matricial:\")\n",
        "print(produto_matricial)\n",
        "\n",
        "# Operações de redução (somando elementos ao longo das colunas)\n",
        "soma_colunas = torch.sum(tensor_a, dim=0)\n",
        "print(\"\\nSoma das Colunas:\")\n",
        "print(soma_colunas)\n",
        "\n",
        "# Operações de broadcast (somando uma constante a cada elemento)\n",
        "tensor_soma_constante = tensor_a + 10\n",
        "print(\"\\nSoma com Constante:\")\n",
        "print(tensor_soma_constante)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyEWNWAamvnZ",
        "outputId": "1459f751-0a23-407f-839a-41e7e4596d1f"
      },
      "id": "lyEWNWAamvnZ",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Soma:\n",
            "tensor([[ 8, 10, 12],\n",
            "        [14, 16, 18]])\n",
            "\n",
            "Subtração:\n",
            "tensor([[-6, -6, -6],\n",
            "        [-6, -6, -6]])\n",
            "\n",
            "Multiplicação (element-wise):\n",
            "tensor([[ 7, 16, 27],\n",
            "        [40, 55, 72]])\n",
            "\n",
            "Produto Matricial:\n",
            "tensor([[ 50,  68],\n",
            "        [122, 167]])\n",
            "\n",
            "Soma das Colunas:\n",
            "tensor([5, 7, 9])\n",
            "\n",
            "Soma com Constante:\n",
            "tensor([[11, 12, 13],\n",
            "        [14, 15, 16]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Os tensores tridimensionais (tensores 3D) são utilizados para representar dados que possuem uma organização tridimensional, como volumes de imagens ou séries temporais multivariadas. Vamos criar alguns exemplos de tensores 3D em PyTorch:"
      ],
      "metadata": {
        "id": "FaxXbiy9rwWK"
      },
      "id": "FaxXbiy9rwWK"
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando um tensor 3D (volume)\n",
        "tensor_3d = torch.tensor([[[1, 2, 3],[4, 5, 6]],\n",
        "                          [[7, 8, 9], [10, 11, 12]],\n",
        "                          [[13, 14, 15], [16, 17, 18]]])\n",
        "\n",
        "# Acessando elementos do tensor 3D\n",
        "print(\"Acessando um elemento:\")\n",
        "print(tensor_3d[1, 0, 2])  # Saída: 9 (depth 1, linha 0, coluna 2)\n",
        "\n",
        "# Operações matemáticas em tensores 3D\n",
        "tensor_resultado = tensor_3d * 2\n",
        "print(\"\\nMultiplicação por 2:\")\n",
        "print(tensor_resultado)\n",
        "\n",
        "# Operações de redução (somando elementos ao longo das dimensões)\n",
        "soma_dim1 = torch.sum(tensor_3d, dim=1)\n",
        "print(\"\\nSoma ao Longo da Dimensão 1:\")\n",
        "print(soma_dim1)\n",
        "\n",
        "# Operações de broadcast (somando uma constante a cada elemento)\n",
        "tensor_soma_constante = tensor_3d + 10\n",
        "print(\"\\nSoma com Constante:\")\n",
        "print(tensor_soma_constante)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKjIklfLnmdS",
        "outputId": "73e44bb5-6ff5-402f-fc43-9dbf30518b70"
      },
      "id": "qKjIklfLnmdS",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acessando um elemento:\n",
            "tensor(9)\n",
            "\n",
            "Multiplicação por 2:\n",
            "tensor([[[ 2,  4,  6],\n",
            "         [ 8, 10, 12]],\n",
            "\n",
            "        [[14, 16, 18],\n",
            "         [20, 22, 24]],\n",
            "\n",
            "        [[26, 28, 30],\n",
            "         [32, 34, 36]]])\n",
            "\n",
            "Soma ao Longo da Dimensão 1:\n",
            "tensor([[ 5,  7,  9],\n",
            "        [17, 19, 21],\n",
            "        [29, 31, 33]])\n",
            "\n",
            "Soma com Constante:\n",
            "tensor([[[11, 12, 13],\n",
            "         [14, 15, 16]],\n",
            "\n",
            "        [[17, 18, 19],\n",
            "         [20, 21, 22]],\n",
            "\n",
            "        [[23, 24, 25],\n",
            "         [26, 27, 28]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neste exemplo, tensor_3d representa um volume 3D com dimensões 3x2x3. Você pode acessar elementos individualmente usando índices em cada dimensão, e as operações matemáticas podem ser realizadas de maneira semelhante aos tensores 2D.\n",
        "\n",
        "Os tensores 3D são frequentemente encontrados em problemas de visão computacional para representar volumes de dados tridimensionais, como pilhas de imagens ou volumes médicos. Esses exemplos básicos podem ser expandidos conforme a complexidade do problema que você está enfrentando."
      ],
      "metadata": {
        "id": "umGcL3m2sY1I"
      },
      "id": "umGcL3m2sY1I"
    },
    {
      "cell_type": "markdown",
      "id": "83cd8734",
      "metadata": {
        "id": "83cd8734"
      },
      "source": [
        "### Derivadas no Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A derivada e a derivada parcial são conceitos fundamentais em cálculo diferencial, e desempenham um papel crucial na compreensão do comportamento de funções em uma ou mais variáveis. Vamos definir ambos os conceitos:\n",
        "\n",
        "1. **Derivada:**\n",
        "   - A derivada de uma função em relação a uma variável representa a taxa de variação instantânea da função em relação a essa variável. Em outras palavras, indica como a função está mudando em um ponto específico do domínio.\n",
        "   - A notação matemática padrão para a derivada de uma função $ f(x) $ em relação a $ x $ é $ f'(x) $ ou $(\\frac{df}{dx})$.\n",
        "   - Geometricamente, a derivada no ponto $ x $ é equivalente à inclinação da tangente à curva da função nesse ponto.\n",
        "   - Exemplo: Imagine que você está dirigindo em um carro e olhando para o velocímetro. A velocidade do carro está mudando constantemente conforme você dirige. A derivada de sua posição em relação ao tempo seria como a leitura instantânea do velocímetro, indicando a velocidade exata em que você está se movendo naquele momento. De maneira mais geral, a derivada de uma função em um ponto nos diz como a função está mudando nesse ponto. Se a função representa a posição de um objeto ao longo do tempo, a derivada seria a velocidade instantânea.\n",
        "\n",
        "2. **Derivada Parcial:**\n",
        "   - A derivada parcial de uma função de várias variáveis em relação a uma variável específica mede a taxa de variação da função em relação a essa variável, mantendo todas as outras variáveis constantes.\n",
        "   - A notação para a derivada parcial de uma função $ f(x, y, \\ldots) $ em relação a uma variável $ x $ é $ \\frac{\\partial f}{\\partial x}$. Aqui, $\\partial$ é usado para indicar uma derivada parcial.\n",
        "   - Analogamente, a derivada parcial em relação a $ y $ seria $ \\frac{\\partial f}{\\partial y} $, e assim por diante.\n",
        "   - Exemplo: Agora, imagine que você está em um terreno acidentado e a temperatura varia em diferentes partes desse terreno. A derivada parcial em relação à temperatura em um determinado ponto seria como medir o quanto a temperatura muda nesse ponto específico, mantendo todas as outras variáveis (como a posição) constantes. Em termos mais gerais, a derivada parcial de uma função de várias variáveis em relação a uma delas nos dá a taxa de variação da função em relação a uma variável específica, mantendo as outras constantes. É como analisar como uma característica específica influencia o comportamento da função, enquanto outras características permanecem inalteradas.\n",
        "\n",
        "Ambos os conceitos, derivada e derivada parcial, ajudam a entender como as coisas mudam em relação a uma variável ou a uma mudança em uma única dimensão, seja em trajetórias de carros ou em paisagens de temperatura. Essas ideias são fundamentais para modelagem matemática e são amplamente aplicadas em diversas áreas, desde física até aprendizado de máquina.\n",
        "\n",
        "A derivada é uma generalização da ideia de taxa de variação para funções de uma única variável, enquanto a derivada parcial lida com funções de várias variáveis, permitindo-nos entender como a função muda ao longo de cada dimensão independente.\n",
        "\n",
        "A fórmula para a derivada parcial em relação a uma variável específica é semelhante à fórmula da derivada para funções de uma variável, considerando todas as outras variáveis como constantes. Essas ferramentas são fundamentais para a análise e otimização de funções em problemas matemáticos e científicos."
      ],
      "metadata": {
        "id": "Sye38FCQlGnq"
      },
      "id": "Sye38FCQlGnq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "A diferenciação automática é uma característica fundamental em PyTorch que permite calcular automaticamente gradientes em relação às variáveis de interesse. Isso é especialmente útil em aprendizado de máquina, onde otimizar modelos frequentemente envolve ajustar parâmetros para minimizar ou maximizar uma função de perda. O mecanismo de diferenciação automática em PyTorch é implementado pelo módulo `autograd`.\n",
        "\n",
        "Vamos ver como realizar diferenciação automática em PyTorch com um exemplo simples:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "# Variáveis que requerem gradiente\n",
        "x = torch.tensor([2.0], requires_grad=True)\n",
        "y = torch.tensor([3.0], requires_grad=True)\n",
        "\n",
        "# Operações envolvendo as variáveis\n",
        "z = x**2 + y**3\n",
        "\n",
        "# Calcular gradientes\n",
        "z.backward()\n",
        "\n",
        "# Exibir gradientes\n",
        "print(\"Gradiente em relação a x:\", x.grad)\n",
        "print(\"Gradiente em relação a y:\", y.grad)\n",
        "\n",
        "Gradiente em relação a x: tensor([4.])\n",
        "Gradiente em relação a y: tensor([27.])\n",
        "```\n",
        "\n",
        "Neste exemplo, `x` e `y` são tensores que requerem gradiente (`requires_grad=True`). A expressão `z = x**2 + y**3` é calculada, e em seguida, chamamos `z.backward()` para calcular automaticamente os gradientes de `z` em relação a `x` e `y`. Os gradientes são então acessados através dos atributos `.grad` das variáveis.\n",
        "\n",
        "Este é um exemplo muito simples, mas em modelos de aprendizado de máquina, esses gradientes são essenciais para a retropropagação (backpropagation) durante o treinamento. Aqui está um exemplo mais elaborado usando uma função de perda e um otimizador:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "# Dados de entrada e saída desejada\n",
        "x_data = torch.tensor([1.0, 2.0, 3.0, 4.0], requires_grad=False)\n",
        "y_data = torch.tensor([2.0, 4.0, 6.0, 8.0], requires_grad=False)\n",
        "\n",
        "# Inicializando parâmetros\n",
        "w = torch.tensor([1.0], requires_grad=True)\n",
        "b = torch.tensor([0.0], requires_grad=True)\n",
        "\n",
        "# Definindo a função linear\n",
        "def modelo_linear(x):\n",
        "    return w * x + b\n",
        "\n",
        "# Função de perda (erro quadrático médio)\n",
        "def loss_function(y_pred, y_actual):\n",
        "    return torch.mean((y_pred - y_actual)**2)\n",
        "\n",
        "# Configurando o otimizador\n",
        "optimizer = optim.SGD([w, b], lr=0.01)\n",
        "\n",
        "# Treinamento do modelo\n",
        "for epoch in range(100):\n",
        "    # Forward pass\n",
        "    y_pred = modelo_linear(x_data)\n",
        "\n",
        "    # Cálculo da perda\n",
        "    loss = loss_function(y_pred, y_data)\n",
        "\n",
        "    # Retropropagação e otimização\n",
        "    optimizer.zero_grad()  # Zera os gradientes acumulados\n",
        "    loss.backward()        # Calcula gradientes\n",
        "    optimizer.step()        # Atualiza parâmetros\n",
        "\n",
        "# Exibindo parâmetros finais\n",
        "print(\"Parâmetros finais: w =\", w.item(), \", b =\", b.item())\n",
        "\n",
        "Parâmetros finais: w = 1.9231737852096558 , b = 0.22587870061397552\n",
        "```\n",
        "\n",
        "Neste exemplo, estamos treinando um modelo linear para ajustar dados de entrada (`x_data`) aos dados de saída desejados (`y_data`). O otimizador (gradiente descendente estocástico - SGD) é utilizado para ajustar os parâmetros `w` e `b` com base no erro da função de perda ao longo de várias iterações.\n",
        "\n",
        "Esses exemplos ilustram como o PyTorch facilita a diferenciação automática, permitindo a criação e treinamento de modelos de aprendizado de máquina de maneira eficiente."
      ],
      "metadata": {
        "id": "577o5SFfkYGz"
      },
      "id": "577o5SFfkYGz"
    },
    {
      "cell_type": "markdown",
      "id": "1577bf91",
      "metadata": {
        "id": "1577bf91"
      },
      "source": [
        "### Conjunto de Dados Simples"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos criar um exemplo básico usando o PyTorch para criar um conjunto de dados (`Dataset`), uma transformação e, em seguida, aplicar transformações adicionais, como normalização e padronização, usando a classe `Compose`. Neste exemplo, usaremos um conjunto de dados fictício representando observações de uma variável.\n",
        "\n",
        "```python\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "# Definindo um conjunto de dados simples\n",
        "class SimpleDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index]\n",
        "\n",
        "# Criando um objeto de conjunto de dados\n",
        "dados = [1.0, 2.0, 3.0, 4.0, 5.0]\n",
        "meu_dataset = SimpleDataset(dados)\n",
        "\n",
        "# Definindo uma transformação simples (multiplicar por 2)\n",
        "class MultiplyTransform:\n",
        "    def __init__(self, factor):\n",
        "        self.factor = factor\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        return sample * self.factor\n",
        "\n",
        "# Criando uma instância da transformação\n",
        "transformacao_multiplica_por_2 = MultiplyTransform(factor=2)\n",
        "\n",
        "# Aplicando a transformação ao conjunto de dados\n",
        "dados_transformados = transformacao_multiplica_por_2(meu_dataset.data)\n",
        "print(\"Dados transformados:\", dados_transformados)\n",
        "\n",
        "# Criando transformações de normalização e padronização\n",
        "transformacao_normalizacao = transforms.Normalize(mean=0, std=1)\n",
        "transformacao_padronizacao = transforms.Standardize()\n",
        "\n",
        "# Compondo as transformações\n",
        "transformacoes_compostas = transforms.Compose([transformacao_multiplica_por_2, transformacao_normalizacao, transformacao_padronizacao])\n",
        "\n",
        "# Aplicando transformações compostas ao conjunto de dados\n",
        "dados_transformados_compostos = transformacoes_compostas(torch.tensor(meu_dataset.data, dtype=torch.float32))\n",
        "print(\"Dados transformados compostos:\", dados_transformados_compostos)\n",
        "```\n",
        "\n",
        "Neste exemplo:\n",
        "\n",
        "- Criamos um conjunto de dados simples chamado `SimpleDataset`.\n",
        "- Definimos uma transformação chamada `MultiplyTransform` que multiplica cada elemento por um fator específico.\n",
        "- Aplicamos essa transformação ao conjunto de dados original.\n",
        "- Em seguida, utilizamos as transformações `Normalize` e `Standardize` fornecidas pelo módulo `transforms` do PyTorch, e as compomos usando `Compose`.\n",
        "- Finalmente, aplicamos as transformações compostas ao conjunto de dados original.\n",
        "\n",
        "Lembrando que as transformações de normalização e padronização são exemplos genéricos e os valores específicos (média e desvio padrão) devem ser ajustados de acordo com a natureza dos dados."
      ],
      "metadata": {
        "id": "gJ3LJIL8uvrX"
      },
      "id": "gJ3LJIL8uvrX"
    },
    {
      "cell_type": "markdown",
      "id": "9e28c20f",
      "metadata": {
        "id": "9e28c20f"
      },
      "source": [
        "### Conjunto de Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos criar um exemplo de `Dataset` para imagens usando `torchvision.transforms` e `torchvision.datasets`. Para isso, precisaremos importar as bibliotecas necessárias e usar um conjunto de dados existente. Neste caso, usaremos o conjunto de dados CIFAR-10 e aplicaremos algumas transformações.\n",
        "\n",
        "Certifique-se de ter o PyTorch e torchvision instalados antes de executar o código.\n",
        "\n",
        "```python\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Definindo um novo conjunto de dados para imagens\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, dataset, transform=None):\n",
        "        self.dataset = dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image, label = self.dataset[index]\n",
        "\n",
        "        # Aplicando transformações, se fornecidas\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Definindo transformações comuns para imagens\n",
        "transformacoes_comuns = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Converte a imagem para tensor\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normaliza os valores dos pixels para o intervalo [-1, 1]\n",
        "])\n",
        "\n",
        "# Baixando o conjunto de dados CIFAR-10\n",
        "conjunto_dados_cifar10 = CIFAR10(root='./data', train=True, download=True)\n",
        "\n",
        "# Criando uma instância do conjunto de dados personalizado com as transformações\n",
        "meu_dataset = CustomImageDataset(dataset=conjunto_dados_cifar10, transform=transformacoes_comuns)\n",
        "\n",
        "# Criando um DataLoader para facilitar o acesso aos dados em lotes\n",
        "loader = DataLoader(meu_dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "# Exibindo alguns exemplos de imagens após as transformações\n",
        "for imagens, labels in loader:\n",
        "    plt.figure(figsize=(10, 2))\n",
        "    for i in range(4):\n",
        "        plt.subplot(1, 4, i + 1)\n",
        "        plt.imshow((imagens[i].numpy().transpose(1, 2, 0) + 1) / 2)  # Desnormaliza e transpõe para exibição correta\n",
        "        plt.title(f\"Classe: {labels[i]}\")\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "    break  # Exibiremos apenas um lote para manter o exemplo conciso\n",
        "```\n",
        "\n",
        "Neste exemplo:\n",
        "\n",
        "- Criamos um conjunto de dados personalizado chamado `CustomImageDataset`, que herda da classe `Dataset`.\n",
        "- Utilizamos o conjunto de dados CIFAR-10 da torchvision.\n",
        "- Definimos transformações comuns usando `torchvision.transforms.Compose`, incluindo a conversão para tensor (`ToTensor()`) e normalização.\n",
        "- Criamos uma instância do conjunto de dados personalizado usando o conjunto de dados CIFAR-10 e aplicamos as transformações.\n",
        "- Utilizamos um `DataLoader` para facilitar o acesso aos dados em lotes.\n",
        "- Exibimos alguns exemplos de imagens após as transformações.\n",
        "\n",
        "Este exemplo serve como ponto de partida para trabalhar com conjuntos de dados de imagens, aplicar transformações e utilizar `DataLoader` para carregar os dados de forma eficiente durante o treinamento de modelos."
      ],
      "metadata": {
        "id": "H-13DD8pzPAD"
      },
      "id": "H-13DD8pzPAD"
    },
    {
      "cell_type": "markdown",
      "id": "2def1558",
      "metadata": {
        "id": "2def1558"
      },
      "source": [
        "## Semana 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08ac4ec1",
      "metadata": {
        "id": "08ac4ec1"
      },
      "source": [
        "### Regressão Linear em 1D"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos criar um exemplo simples de regressão linear usando PyTorch, construindo uma classe linear básica e outra usando `nn.Module` para encapsular o modelo. Neste exemplo, usaremos dados de treinamento gerados aleatoriamente."
      ],
      "metadata": {
        "id": "SwnMUGTg9idz"
      },
      "id": "SwnMUGTg9idz"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Gerando dados de treinamento\n",
        "np.random.seed(42)\n",
        "x_train = np.random.rand(100, 1) * 10\n",
        "y_train = 2 * x_train + 1 + 2 * np.random.randn(100, 1)\n",
        "\n",
        "# Convertendo para tensores do PyTorch\n",
        "x_train_tensor = torch.from_numpy(x_train).float()\n",
        "y_train_tensor = torch.from_numpy(y_train).float()\n",
        "\n",
        "# Definindo uma classe para regressão linear\n",
        "class LinearRegressionModel(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(LinearRegressionModel, self).__init__()\n",
        "        self.linear = nn.Linear(input_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "# Criando uma instância do modelo\n",
        "input_size = 1  # Uma variável de entrada\n",
        "output_size = 1  # Uma saída\n",
        "modelo_regressao_linear = LinearRegressionModel(input_size, output_size)\n",
        "\n",
        "# Definindo a função de perda (Mean Squared Error)\n",
        "criterio = nn.MSELoss()\n",
        "\n",
        "# Definindo o otimizador (Gradiente Descendente Estocástico)\n",
        "otimizador = optim.SGD(modelo_regressao_linear.parameters(), lr=0.01)\n",
        "\n",
        "# Treinando o modelo\n",
        "num_epochs = 1000\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    y_pred = modelo_regressao_linear(x_train_tensor)\n",
        "\n",
        "    # Calculando a perda\n",
        "    loss = criterio(y_pred, y_train_tensor)\n",
        "\n",
        "    # Retropropagação e otimização\n",
        "    otimizador.zero_grad()\n",
        "    loss.backward() # calcula os gradientes\n",
        "    otimizador.step() # atualiza os pesos do modelo\n",
        "\n",
        "    # Exibindo a perda a cada 100 épocas\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f'Época [{epoch + 1}/{num_epochs}], Perda: {loss.item():.4f}')\n",
        "\n",
        "# Visualizando os resultados\n",
        "modelo_regressao_linear.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred_final = modelo_regressao_linear(x_train_tensor)\n",
        "\n",
        "plt.scatter(x_train, y_train, label='Dados de Treinamento')\n",
        "plt.plot(x_train, y_pred_final.numpy(), color='red', label='Predições do Modelo')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623
        },
        "id": "08yFjuqz9dRx",
        "outputId": "cbe31a77-a69f-43fa-9a2f-c2827f511536"
      },
      "id": "08yFjuqz9dRx",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época [100/1000], Perda: 3.4729\n",
            "Época [200/1000], Perda: 3.3074\n",
            "Época [300/1000], Perda: 3.2530\n",
            "Época [400/1000], Perda: 3.2351\n",
            "Época [500/1000], Perda: 3.2292\n",
            "Época [600/1000], Perda: 3.2273\n",
            "Época [700/1000], Perda: 3.2266\n",
            "Época [800/1000], Perda: 3.2264\n",
            "Época [900/1000], Perda: 3.2264\n",
            "Época [1000/1000], Perda: 3.2263\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjIAAAGwCAYAAACzXI8XAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABVFElEQVR4nO3deVxU9foH8M+AsgmM4sKAgeBSQipuqag3K/GKppeWW9bPtczbRbuZVlrd3HIhbLdMzVIrte1WLpSWK6bhFmHimoRpCpqoICioM+f3B87EMGdmzgwzc86Z+bxfL14v58yZM98Z1Hnm+T7f56sRBEEAERERkQr5yT0AIiIiImcxkCEiIiLVYiBDREREqsVAhoiIiFSLgQwRERGpFgMZIiIiUi0GMkRERKRa9eQegLsZDAacPn0aYWFh0Gg0cg+HiIiIJBAEAZcuXUJ0dDT8/KznXbw+kDl9+jRiYmLkHgYRERE54eTJk7jpppus3u/1gUxYWBiA6jciPDxc5tEQERGRFGVlZYiJiTF9jlvj9YGMcTopPDycgQwREZHK2CsLYbEvERERqRYDGSIiIlItBjJERESkWl5fIyOVXq/HtWvX5B4GkeIFBATYXApJRORJPh/ICIKA4uJiXLx4Ue6hEKmCn58f4uPjERAQIPdQiIgYyBiDmGbNmiEkJIRN84hsMDaYLCoqQmxsLP+9EJHsfDqQ0ev1piCmcePGcg+HSBWaNm2K06dP4/r166hfv77cwyEiH+fTE93GmpiQkBCZR0KkHsYpJb1eL/NIiIh8PJAxYnqcSDr+eyEiJfHpqSUiIiJyjt4gYHfheZy9VIlmYUHoFh8Bfz/Pf9FhIENEREQOWZ9fhBlrD6KotNJ0LEobhGmDE5HaLsqjY+HUEllYtmwZGjZs6NHnnD59Ojp27OjR57Rn69at0Gg0XJpPRFTD+vwipC/PNQtiAKC4tBLpy3OxPr/Io+NhIOMCeoOAnIISrM47hZyCEugNglufb9SoUdBoNNBoNKhfvz4iIyPRr18/LFmyBAaDwa3PrVTG98Paz/Tp0x2+Zs+ePVFUVAStVuv6AcuAgRkR1ZXeIGDG2oMQ+5QzHpux9qDbPwdr4tRSHcmVXktNTcXSpUuh1+tx5swZrF+/HuPHj8f//vc/rFmzBvXq+davtqjor28An332GaZOnYojR46YjoWGhpr+LAgC9Hq93fcoICAAOp3O9YMlIlKp3YXnLTIxNQkAikorsbvwPJJbeaatCTMydSBnei0wMBA6nQ7NmzdH586d8cILL2D16tVYt24dli1bZjrv9ddfR/v27dGgQQPExMRg7NixKC8vN7vWsmXLEBsbi5CQENx7770oKSmxeL4FCxagVatWCAgIwC233IKPP/7YdJ8gCJg+fTpiY2MRGBiI6OhoPPnkkzbH//LLLyMyMhJhYWEYPXo0Kist/2G8//77SEhIQFBQENq2bYt3333X6vV0Op3pR6vVQqPRmG4fPnwYYWFhWLduHbp06YLAwEBs374dBoMBGRkZiI+PR3BwMJKSkvC///3PdM3aGQzjlNt3332HhIQEhIaGIjU11SyI2rNnD/r164cmTZpAq9WiT58+yM3NNRurRqPBokWLMGjQIISEhCAhIQE5OTk4duwY7rjjDjRo0AA9e/ZEQUGB2eNWr16Nzp07IygoCC1btsSMGTNw/fp1s+u+//77uPfeexESEoI2bdpgzZo1AIDjx4/jzjvvBAA0atQIGo0Go0aNAgBUVVXhySefRLNmzRAUFITevXtjz549Nn9/RGSbpzP1nnL2kvUgxpnzXIGBjJOUmF676667kJSUhK+++sp0zM/PD/PmzcOBAwfw4YcfYvPmzZg0aZLp/l27dmH06NF44oknkJeXhzvvvBOzZs0yu+7XX3+N8ePH4+mnn0Z+fj4ef/xxPPLII9iyZQsA4Msvv8Qbb7yBRYsW4ddff8WqVavQvn17q+P8/PPPMX36dMyZMwd79+5FVFSURZCyYsUKTJ06FbNnz8ahQ4cwZ84cTJkyBR9++KHT789zzz2Hl19+GYcOHUKHDh2QkZGBjz76CAsXLsSBAwcwYcIEDBs2DNnZ2VavcfnyZbz66qv4+OOPsW3bNpw4cQLPPPOM6f5Lly5h5MiR2L59O3bu3Ik2bdpg4MCBuHTpktl1Zs6ciREjRiAvLw9t27bF//3f/+Hxxx/H888/j71790IQBDzxxBOm83/44QeMGDEC48ePx8GDB7Fo0SIsW7YMs2fPNrvujBkz8OCDD+KXX37BwIEDMXToUJw/fx4xMTH48ssvAQBHjhxBUVER3nrrLQDApEmT8OWXX+LDDz9Ebm4uWrdujf79++P8+fNOv9dEvmx9fhF6Z27Gw4t3YvyneXh48U70ztzs8doRd2gWFuTS81xC8HKlpaUCAKG0tNTivitXrggHDx4Urly54vB1fzx2TmgxOcvuz4/HzrniZZgZOXKkkJaWJnrfkCFDhISEBKuP/eKLL4TGjRubbj/88MPCwIEDLa6h1WpNt3v27CmMGTPG7JwHHnjA9LjXXntNuPnmm4WrV69KGn9ycrIwduxYs2Pdu3cXkpKSTLdbtWolrFy50uycmTNnCsnJyXavv3TpUrPxb9myRQAgrFq1ynSssrJSCAkJEX788Uezx44ePVp4+OGHzR534cIF03UBCMeOHTOdP3/+fCEyMtLqWPR6vRAWFiasXbvWdAyA8OKLL5pu5+TkCACEDz74wHTsk08+EYKCgky3+/btK8yZM8fs2h9//LEQFRVl9brl5eUCAGHdunWir8d4Tv369YUVK1aYjl29elWIjo4W5s6dK/qa6vLvhsjbrdt/WogT+SyIu/Gzbv9puYdYJ9f1BqHHnI2ir9H4OnvM2Shc1xvq/Fy2Pr9rYkbGSUpMrwHV0zw1G5Zt3LgRffv2RfPmzREWFobhw4ejpKQEly9fBgAcOnQI3bt3N7tGcnKy2e1Dhw6hV69eZsd69eqFQ4cOAQAeeOABXLlyBS1btsSYMWPw9ddfm0151GbvOSsqKlBQUIDRo0cjNDTU9DNr1iyL6RZHdO3a1fTnY8eO4fLly+jXr5/Zc3z00Uc2nyMkJAStWrUy3Y6KisLZs2dNt8+cOYMxY8agTZs20Gq1CA8PR3l5OU6cOGF2nQ4dOpj+HBkZCQBmWazIyEhUVlairKwMALBv3z689NJLZmMdM2YMioqKTL/L2tdt0KABwsPDzcZXW0FBAa5du2b2+61fvz66detm+v0SkTRKzNS7mr+fBtMGJwIAaneMMd6eNjjRo/1kfKsi1IUUmV5DdZAQHx8PoLouYtCgQUhPT8fs2bMRERGB7du3Y/To0bh69arLtmaIiYnBkSNHsHHjRmzYsAFjx47FK6+8guzsbKf24jHW8CxevNgi4PH393d6nA0aNLB4jm+++QbNmzc3Oy8wMNDqNWq/Ho1GA0H46z+lkSNHoqSkBG+99RZatGiBwMBAJCcn4+rVq1avYww8xY4ZV6GVl5djxowZuO+++yzGFBT0198xsfH56ko2Ik9TYiGsO6S2i8KCYZ0tFrroZOojw0DGSd3iIxClDUJxaaVo9K1B9S+1W3yEx8a0efNm7N+/HxMmTAAA/PTTTzAYDHjttdfg51edfPv888/NHpOQkIBdu3aZHdu5c6fFOTt27MDIkSNNx3bs2IHExETT7eDgYAwePBiDBw/GuHHj0LZtW+zfvx+dO3e2GKfxOUeMGCH6nJGRkYiOjsZvv/2GoUOHOvo2SJKYmIjAwECcOHECffr0cdl1d+zYgXfffRcDBw4EAJw8eRLnzp2r83U7d+6MI0eOoHXr1k5fQ2yPJGMB944dO9CiRQsA1XuQ7dmzB0899VSdxkzka5SaqXeH1HZR6JeoY2dfNTOm19KX50IDmAUznkivVVVVobi42Gz5dUZGBgYNGmQKEFq3bo1r167h7bffxuDBg7Fjxw4sXLjQ7DpPPvkkevXqhVdffRVpaWn47rvvsH79erNznn32WTz44IPo1KkTUlJSsHbtWnz11VfYuHEjgOrVPHq9Ht27d0dISAiWL1+O4OBg0wdjbePHj8eoUaPQtWtX9OrVCytWrMCBAwfQsmVL0zkzZszAk08+Ca1Wi9TUVFRVVWHv3r24cOECJk6cWOf3LywsDM888wwmTJgAg8GA3r17o7S0FDt27EB4eLhZ0OaINm3a4OOPP0bXrl1RVlaGZ599FsHBwXUe79SpUzFo0CDExsbin//8J/z8/LBv3z7k5+dbFGdb06JFC2g0GmRlZWHgwIEIDg5GaGgo0tPT8eyzzyIiIgKxsbGYO3cuLl++jNGjR9d53ES+RKmZenfx99MoIrPEGpk6MKbXdFrzv5Q6bRAWDOvs1vTa+vXrERUVhbi4OKSmpmLLli2YN28eVq9ebZp+SUpKwuuvv47MzEy0a9cOK1asQEZGhtl1evTogcWLF+Ott95CUlISvv/+e7z44otm59xzzz1466238Oqrr+LWW2/FokWLsHTpUtxxxx0AgIYNG2Lx4sXo1asXOnTogI0bN2Lt2rVo3Fj8L/iQIUMwZcoUTJo0CV26dMHvv/+O9PR0s3Mee+wxvP/++1i6dCnat2+PPn36YNmyZaZpM1eYOXMmpkyZgoyMDCQkJCA1NRXffPNNnZ7jgw8+wIULF9C5c2cMHz7ctKy5rvr374+srCx8//33uO2229CjRw+88cYbVoNFMc2bN8eMGTPw3HPPITIy0rQq6uWXX8b999+P4cOHo3Pnzjh27Bi+++47NGrUqM7jJvIlxky9ta+vGlT3GfNkpt4XaISaE/xeqKysDFqtFqWlpQgPDze7r7KyEoWFhYiPjzerM3CUUjbOIvIEV/27IfJGxv5igHim3t1fcr2Jrc/vmji15AJKSa8REZG8lFYI6wsYyBAREbmQkgphfQEDGSIiIhdjpt5zWOxLREREqsVAhoiIiFSLU0tEREQkSg2rchnIEBERkYX1+UUWq6+iFLj6ilNLREREZMbYD6f23lHFpZVIX56L9flFMo3MEgMZsmnUqFG45557TLfvuOMOh/bgKSgoQPPmzTFgwAD8+eefaNeunesHaYejY5bL9OnT0bFjR8nnHz9+HBqNBnl5eW4bExH5HrXt4s1ARoVGjRoFjUYDjUaDgIAAtG7dGi+99BKuX7/u9uf+6quvMHPmTMnnr1+/HuPGjUO/fv3QrVs3PProo24cnfvExcVBo9Hg008/tbjv1ltvhUajwbJlyzw/MCIiF3NkF28lYI2MSqWmpmLp0qWoqqrCt99+i3HjxqF+/fp4/vnnLc69evWqaefjuoqIcGyPkHHjxpn+7IrNHuUUExODpUuX4qGHHjId27lzJ4qLi9GgQQMZR0ZEvsITxbdq28WbGRmVCgwMhE6nQ4sWLZCeno6UlBSsWbMGwF/TQbNnz0Z0dDRuueUWAMDJkyfx4IMPomHDhoiIiEBaWhqOHz9uuqZer8fEiRPRsGFDNG7cGJMmTULtrbhqT9NUVVVh8uTJiImJQWBgIFq3bo0PPvjAdH92dja6deuGwMBAREVF4bnnnjPLHBkMBmRkZCA+Ph7BwcFISkrC//73P9P9Fy5cwNChQ9G0aVMEBwejTZs2WLp0qdX3paKiAiNGjEBoaCiioqLw2muvWZxz4cIFjBgxAo0aNUJISAgGDBiAX3/91e57PnToUGRnZ+PkyZOmY0uWLMHQoUNRr575d4ITJ04gLS0NoaGhCA8Px4MPPogzZ86YnfPyyy8jMjISYWFhGD16NCorLf9TeP/995GQkICgoCC0bdsW7777rs0x2nu/iUiZ9AYBOQUlWJ13CjkFJaLTNuvzi9A7czMeXrwT4z/Nw8OLd6J35maX16uobRdvBjI1CQJQUSHPTx337gwODsbVq1dNtzdt2oQjR45gw4YNyMrKwrVr19C/f3+EhYXhhx9+wI4dOxAaGorU1FTT41577TUsW7YMS5Yswfbt23H+/Hl8/fXXNp93xIgR+OSTTzBv3jwcOnQIixYtQmhoKADg1KlTGDhwIG677Tbs27cPCxYswAcffIBZs2aZHp+RkYGPPvoICxcuxIEDBzBhwgQMGzYM2dnZAIApU6bg4MGDWLduHQ4dOoQFCxagSZMmVsfz7LPPIjs7G6tXr8b333+PrVu3Ijc31+ycUaNGYe/evVizZg1ycnIgCAIGDhyIa9eu2XytkZGR6N+/Pz788EMAwOXLl/HZZ59ZTJcZDAakpaXh/PnzyM7OxoYNG/Dbb79hyJAhpnM+//xzTJ8+HXPmzMHevXsRFRVlEaSsWLECU6dOxezZs3Ho0CHMmTMHU6ZMMT1/bVLebyJSHikBiieLb1W3i7fg5UpLSwUAQmlpqcV9V65cEQ4ePChcuXKl+kB5uSBUhxSe/ykvl/yaRo4cKaSlpQmCIAgGg0HYsGGDEBgYKDzzzDOm+yMjI4WqqirTYz7++GPhlltuEQwGg+lYVVWVEBwcLHz33XeCIAhCVFSUMHfuXNP9165dE2666SbTcwmCIPTp00cYP368IAiCcOTIEQGAsGHDBtFxvvDCCxbPOX/+fCE0NFTQ6/VCZWWlEBISIvz4449mjxs9erTw8MMPC4IgCIMHDxYeeeQRSe/LpUuXhICAAOHzzz83HSspKRGCg4NNYz569KgAQNixY4fpnHPnzgnBwcFmj6utRYsWwhtvvCGsWrVKaNWqlWAwGIQPP/xQ6NSpkyAIgqDVaoWlS5cKgiAI33//veDv7y+cOHHC9PgDBw4IAITdu3cLgiAIycnJwtixY82eo3v37kJSUpLpdqtWrYSVK1eanTNz5kwhOTlZEARBKCwsFAAIP//8syAI9t9vV7H4d0NETlu3/7QQNzlLaFHrJ+7Gz7r9p4XreoPQY85Gi3Nqnttjzkbhut5g/wkdHFftsdUcl7vZ+vyuiRkZlcrKykJoaCiCgoIwYMAADBkyBNOnTzfd3759e7O6mH379uHYsWMICwtDaGgoQkNDERERgcrKShQUFKC0tBRFRUXo3r276TH16tVD165drY4hLy8P/v7+6NOnj+j9hw4dQnJyMjSav+L6Xr16oby8HH/88QeOHTuGy5cvo1+/fqYxhYaG4qOPPkJBQQEAID09HZ9++ik6duyISZMm4ccff7Q6noKCAly9etXsNURERJim1oxjqlevntk5jRs3xi233IJDhw5ZvbbR3XffjfLycmzbtg1LliwRLV4+dOgQYmJiEBMTYzqWmJiIhg0bmp7j0KFDZmMAgOTkZNOfKyoqUFBQgNGjR5u9N7NmzTK9N2LPa+v9JiJlkbo6aGdBiceLb427eOu05tNHOm0QFgzrrKg+Miz2rSkkBCgvl++5HXDnnXdiwYIFCAgIQHR0tEWNRu3i0/LycnTp0gUrVqywuFbTpk0dHy+qp7PqovzGe/3NN9+gefPmZvcFBgYCAAYMGIDff/8d3377LTZs2IC+ffti3LhxePXVV+v03M6qV68ehg8fjmnTpmHXrl12p96cZXxvFi9ebBHw+Pv7u+U5icizpK4OyvntnKTrubr4Vi27eDOQqUmjAVSy+qRBgwZo3bq15PM7d+6Mzz77DM2aNUN4eLjoOVFRUdi1axduv/12AMD169fx008/oXPnzqLnt2/fHgaDAdnZ2UhJSbG4PyEhAV9++SUEQTBlCXbs2IGwsDDcdNNNaNSoEQIDA3HixAmrWR2gOtAaOXIkRo4cib/97W949tlnRQOZVq1aoX79+ti1axdiY2MBVBf2Hj161HT9hIQEXL9+Hbt27ULPnj0BACUlJThy5AgSExOtjqGmRx99FK+++iqGDBmCRo0aib7ukydP4uTJk6aszMGDB3Hx4kXTcyQkJGDXrl0YMWKE6XE7d+40/TkyMhLR0dH47bffMHToUEnjsvd+E5GySA88pAUO7ii+VcMu3pxa8hFDhw5FkyZNkJaWhh9++AGFhYXYunUrnnzySdO0w/jx4/Hyyy9j1apVOHz4MMaOHYuLFy9avWZcXBxGjhyJRx99FKtWrTJd8/PPPwcAjB07FidPnsR//vMfHD58GKtXr8a0adMwceJE+Pn5ISwsDM888wwmTJiADz/8EAUFBcjNzcXbb79tKmidOnUqVq9ejWPHjuHAgQPIyspCQkKC6HhCQ0MxevRoPPvss9i8eTPy8/MxatQo+Pn99de8TZs2SEtLw5gxY7B9+3bs27cPw4YNQ/PmzZGWlibpvUxISMC5c+esrp5KSUlB+/btMXToUOTm5mL37t0YMWIE+vTpY5qqGz9+PJYsWYKlS5fi6NGjmDZtGg4cOGB2nRkzZiAjIwPz5s3D0aNHsX//fixduhSvv/666PPae7+JSFmkBh7JrRqrq/jWw/i/m48ICQnBtm3bEBsbi/vuuw8JCQmmJb/GDM3TTz+N4cOHY+TIkUhOTkZYWBjuvfdem9ddsGAB/vnPf2Ls2LFo2bIlxowZg4qKCgBA8+bN8e2332L37t1ISkrCv//9b4wePRovvvii6fEzZ87ElClTkJGRgYSEBKSmpuKbb75BfHw8ACAgIADPP/88OnTogNtvvx3+/v6iTemMXnnlFfztb3/D4MGDkZKSgt69e6NLly5m5yxduhRdunTBoEGDkJycDEEQ8O2336J+/fqS38/GjRtbnVrTaDRYvXo1GjVqhNtvvx0pKSlo2bIlPvvsM9M5Q4YMwZQpUzBp0iR06dIFv//+O9LT082u89hjj+H999/H0qVL0b59e/Tp0wfLli0zvTe1SXm/iUg5pK4O6tGyMaYNTjQdq30OAEwbnKi4KR9P0QhCHdf9KlxZWRm0Wi1KS0stplQqKytRWFiI+Ph4BAUpYz28mj3++ON48MEH0bdvX7mHQm7EfzdErmNcVg3ArOjXGJLULKxVyyaOrmLr87sm1shQnZWWluLcuXMICAjAmjVrGMgQEUlkXB1UO0DRiQQoiiy+LS4Gfv8d6Natus5UBgxkqM5OnTqFHj16ICgoCMuXL5d7OEREquJIgOJs8a3LtzYQBGDQIODbb6tvr1sHpKY6f706kDWQycjIwFdffYXDhw8jODgYPXv2RGZmplnfj8rKSjz99NP49NNPUVVVhf79++Pdd99FZGSkjCOnmhITE1FWVib3MIiIVMudq4NcPiW1YQPw97+bH7PRc8zdZC32zc7Oxrhx47Bz505s2LAB165dw9///ndTsSgATJgwAWvXrsUXX3yB7OxsnD59Gvfdd5+MoyYiIlIHW1sb/Ht5Ll5ae8Dq3k4WLl8GGjY0D2JCQoBLlwAbW8e4m6KKff/88080a9YM2dnZuP3221FaWoqmTZti5cqV+Oc//wkAOHz4MBISEpCTk4MePXrYvaaUYt+4uLg6N3cj8hVXrlzB8ePHWexLpHB6g4DemZttNt0zspuheeMNYOJE82PffgsMGOCCkYqTWuyrqOXXpaWlAKrbygPATz/9hGvXrpk1W2vbti1iY2ORk5Mjeo2qqiqUlZWZ/VhjXG57+fJlV70EIq9n3GSUHYaJlM1e5+CarG4++dVX1UW8NYOYfv0Avd6tQYwjFFPsazAY8NRTT6FXr15o164dAKC4uBgBAQFo2LCh2bmRkZEoLi4WvU5GRgZmzJgh6Tn9/f3RsGFDnD17FkB1rxWNTFXXRGpgMBjw559/IiQkxGJbDCJSFke2LBBQveR7xtqD6Jeog78GgFgjzcOHgRp1rEqgmP+Jxo0bh/z8fGzfvr1O13n++ecxsUbkWFZWZrZ5X206nQ4ATMEMEdnm5+eH2NhYBv1ECmFtRZKjWxYY93ZCeBhQo1YVANC7N/DDD64btAspIpB54oknkJWVhW3btpntCaPT6XD16lVcvHjRLCtz5swZUwBSW2BgoGnDQSk0Gg2ioqLQrFkzXLt2zenXQOQrAgICuOUBkULYWpHUL1GHKG0QiksrRXfYrk1Xdg47F4yyvGP/fuDGTIkSyRrICIKA//znP/j666+xdetWi9brXbp0Qf369bFp0ybcf//9AIAjR47gxIkTSE5OdulY/P39OedPRESqYVyRVDtIMda7LBjWGdMGJyJ9eS40gM1g5njmIPE7lLMeyCpZv1aNGzcOy5cvx8qVKxEWFobi4mIUFxfjypUrAACtVovRo0dj4sSJ2LJlC3766Sc88sgjSE5OlrRiiYiIyBvpDQJmrD0oGpwYjxnrXRYM6wydVnyaaci+78SDmMuXVRHEADJnZBYsWAAAuOOOO8yOL126FKNGjQIAvPHGG/Dz88P9999v1hCPiIjIV9lbkWSsd9ldeN6sc/DGg8X4YMdxAOJZmCPJfXHLjxvdNGr3UFQfGXeQug6diIhILVbnncL4T/PsnvfWQx2R1rG5+UErhfpxk7OggflGlXJSZR8ZIiIipdAbBOQUlGB13inp3W89ROqKJLPziotFg5gnBz+DuMlZptsz1h5U1Gu1RxGrloiIiJTE5fsTuVi3+AibK5I0qN5Bu1t8xI0D1rMwNdWcknLX3k+uxowMERHVmZKzF46ytT+RaPdbGfj7aTBtcCKA6qClJuPtaYMT4T97lmgQk/TkJxZBTE2ONNOTGzMyRERUJ0rPXjjC3mogs+63fvI2hUxtF4UFwzpbvPc643vfPlr0cbYCGCNHm+nJiYEMERE5TUovEzUFM46sBlLC1EvNFUnGzr7JrZsAL4icLAjQGwREZW6WPiWlApxaIiIip0jtZaKmaSapUypKmnrx99MguVVjpMXcCGJqe+EFU08YyVNSMmebHMFAhoiInOJI9kItnFoNpAQaDdBEJIgRBGD2bLNDximp2k3ydNog1WXQAE4tERGRk9SYvbDH4dVAcps7F5g82fJ4QQHQsqXVh4lNSRk3m1QbBjJEROQU1WYvbDBOvYjtT6S4qRdrO9BL7HNrnJJSO04tERGRU4zZC2sf6RpUr15STPZCIsVPvWg04kGMIKhmfyRXYkaGiIicoqrshYMUOfVSVgZotZbHU1KADRs8Px6F4F5LRERUJ97UR0ax6jiNpEZSP7+ZkSEiojpRZPbCW8yfDzzxhOXxLVuAO+5w61PrDYIqfqcMZIiIqM68pXBUUWTMwqgpy8ZiXyIiIiWxVsyr13ssiFH6XlM1MZAhIiJSgqtXxQMYjaY6gPFz/0e2Grs1M5AhIiKSm0YDBAZaHhcEwGDw2DDU2K2ZgQwREZFcli8Xz8J8+qksK5LU2K2Zxb5ERERyUOCSajV2a2YgQ0REbqX0ZbweH5+1AObKFSBI3gBBdXtNgYEMERG5kdKX8Xp0fAYD4O8vfl+NLIycgZ8auzWzsy8REbmFcRlv7Q8Z40eg3PsWeXR8EqeRlBL4KWEcUj+/GcgQEZHL6Q0CemdutroCxjhFsX3yXbJ8u/fY+L75Bhg0yPL4jBnA1Klmh5QW+Mk9JcgtCoiISDaOLOOVoyOwR8bnQDGvvf4tGlT3b+mXqPPoNJMaujVz+TUREbmc0pfxunV81jrzlpRYXZGkxv4tSsGMDBERuZzSl/G6bXxOLqlWeuCnZMzIEBGRyxmX8VqbBNGgunhUrmW8Lh+ftSyMIEjqC6P0wE/JGMgQEZHLGZfxArAIFuRYxqs3CMgpKMHqvFPIKSgxPX+dx5eTIx7A/O1vDjW2U3rgp2RctURERG6jhGW8tsYAwPnxubgzr3HVEiDev0Xu5eqexuXXNzCQISKSl5zLeKUsae6XqHNsfE2aVBfu1nbkCHDzzWaHHH3tSgj8lIKBzA0MZIiIfJNbesU4kIVxNiiRu3+LUkj9/GaNDBEReSWpS5p3/iaSXanNwWJeYyao9vMXl1YifXku1ucXWX0qY/+WtI7NkdyqsU8GMY5gIENERF5J6lLlcStsBBa//ioewEREWK2FsdfcDqiuy9EbvHpCxGMYyBARkVeSulT54pVr4lkSjcai5gVAdQAjViNzA5vbeRYDGSIimdVeGsxv6q5hb0lzbaYsSVKSeBZmxw5JK5LY3M6z2NmXiEhGXKXiPsZeNsYlzbYYsyT+/la+3zuwLobN7TyLGRkiIpnUpSCUpEltF4UFwzqjYXB9m+cdzxyE45kiu1RL7MxbE5vbeRYDGSIiGbAg1HNS20Vh/v91Fr2vafl58QAGcLqxndK6Gns7BjJERDJgQahn9WjV2CJLcjxzEPbMH2F5shNZmNqMmSCd1nz6SKcN8rkOve7GGhkiIhmwINSzatbLLP1iGu787SeLc/bPeBXtpz5tdqwuzelS20U53jWYHMZAhohIBiwI9bzUdlEotDKNtH7/aYssiSsKsY3N7ch9OLVERCQDFoR6mJXOvDlHzkCvN4gGMb5ciK2mlgDMyBARyaDmVIcG4rsdsyDUBcrLgbAw8fsEAckih+0VYmtQXYjdL1Hnlb8ftbUEYEaGiEgmLAh1M41GPIixU8zry4XYasxEMSNDRCQjXygI9fhuzmPHAgsWWB5/8kngrbfsPtxXC7HVmoliIENEJDNvLgj1+DSF2NYCADvzSuBIJkpJf185tURERG7h0WkKK8W8KC9nZ16J1JqJYiBDREQu57HOxdev287CNGjg8CV9tTOvWjNRDGSIiMjlPFIwq9EA9UX2UGJnXqeoNRPFGhkiInI5t05TvPkmMGGC5fGuXYE9exy/nhW+UIhdk1pbAjCQISIil3PbNIWNaSS9QcDughKXBh3eXIgtxpiJql2grVNwHxkGMkRE5HLGaYri0krROhkNqj8cJU9TWAtgiooAnU51TdyUTG2ZKNbIEBGRy9UsmBUjAPhHUpTND0djm3ybxbw3ghi1NXFTOmMmKq1jcyS3aqzYIAZgIENERG6S2i4K/7o93ur9720rtBpkrM8vgr+/H5JbN7G8s0Yxr8dWR5FiMZAhIiKn2NtYUG8QsGaf7WyIWJDxy5x5SG0fLXp+/OQss+DHl7cToGqskSEiIodJqUlxqlOsRoMOIufGTc6qvhvmbfLV2sSNXIcZGSIicojUmhSHggwrnXnThr9mCmIAywyLWpu4keswkCEiIskcqUmRGjykdbpJ9Hjc5Czsi75F9D5jkKTWJm7kOpxaIiIiyRyZLrK3BPt45iDRa9TMwFjTJDQQOTd6xjx0Wwze2Pirqpq4keswkCEiIskcmS6y1in2tpP5+GLlc6KP0+sNiMrcbLP/TMOQ+nj68zwUl1WZjjcMqd6q4OLla6ZjSm7iRq7DQIaIiCRztCaldqdYa1kY43Jqf8Bmm3wBwIUawYpR6Y1jE1JuRlyTEMU3cSPXYY0MERFJ5kxNSmq7KOS8kCIexHzxhcUGj7Y2bDRmXmozXuHTPScwqEO04pu4keswI0NERJI5tbGgrc68Voi1yTcYBAz9YJfVx4gu5yavx4wMERE5xFbGZMGwzn/VpFhZUl2zM68ttdvkbzp8RtL42DPGtzAjQ0REDrO5seDRo8At4sumpQQwYtbnF2HJjuOSzmXPGN8ia0Zm27ZtGDx4MKKjo6HRaLBq1Sqz+0eNGgWNRmP2k5qaKs9giYjIjOjGghqNeBAjMQsjxti7Rgr2jPE9sgYyFRUVSEpKwvz5862ek5qaiqKiItPPJ5984sEREhGRJNamkaZOdTqAMbLXu6Ym9ozxPbJOLQ0YMAADBgyweU5gYCB0Op3ka1ZVVaGq6q/eAmVlZU6Pj4iIJHCimNcRUmteRveKY88YH6T4Yt+tW7eiWbNmuOWWW5Ceno6SkhKb52dkZECr1Zp+YmJiPDRSIiIfYy0Lo9e7LIgBpNe8pCRK/9JL3kPRgUxqaio++ugjbNq0CZmZmcjOzsaAAQOg1+utPub5559HaWmp6efkyZMeHDERkQ84f952FsbPtR8t3E+JbFH0qqWHHnrI9Of27dujQ4cOaNWqFbZu3Yq+ffuKPiYwMBCBgYGeGiIRkW9x8zSSGGPvmn8vzxV/arA2xpcpOiNTW8uWLdGkSRMcO3ZM7qEQkRfSGwTkFJRgdd4p5BSUQG9w34ez6nTrJh7EDB7s1iCGyB5FZ2Rq++OPP1BSUoKoKBZzEZFrrc8vMu0HZBSlgE0H9QZBvFeLJ8mQhanJ3vJrDYAZaw+iX6KOWRkfJGsgU15ebpZdKSwsRF5eHiIiIhAREYEZM2bg/vvvh06nQ0FBASZNmoTWrVujf//+Mo6aiLzN+vwipC/Ptdhtubi0EunLc8271Xp4XLIGV9YCmEuXgNBQ9z//DfaWX3NrAt8m69TS3r170alTJ3Tq1AkAMHHiRHTq1AlTp06Fv78/fvnlF/zjH//AzTffjNGjR6NLly744YcfWANDRC5j/LYvllswHpux9qDHp5mMwVXtD3BjcLU+v8gtz6s3CNh5qMh2FsaDQQwgffk1tybwTbJmZO644w4INlKT3333nQdHQ0S+SInf9u0FV+6aSlmfX4TU9tHoIXanjHUwUpdfc2sC36SqYl8iIldT4rd9R4IrV/njniFIbR9tcfxsg0aIn5wlmgHyVHE0l1+TLaoq9iUicjUlftv3eHCl0eAmkcNxk7Oq74ZlBsiT9TvG5dfpy3OhAcwyVcbghsuvfRczMkTk05T4bd9jwZWVzrw905eYghjAMgMkR/1OarsoLBjWGTqt+WvWaYNkK8YmZWBGhoh8mhK/7RuDq+LSStE6GQ2qP8CdDq5sdN+tGcDUdvZSpWz1O0B1MNMvUYedv5Ugp6AEgIDklk3QgyuVfBozMkTk85T2bd8YXAGwyBTVObjSaESDmLjJWTaDGKA6AyRH/U5NGw4W45kv9uGdLcfwzpYCDP1gF3pnbnbbKi5SPmZkiIjw17d9OZvP1W5+N///OmPmN+Z1KDpn61AyM4HnnhN/Xr0BUZmbJWWAsn45Lenp3FEcrdR+PyQvBjJERDf4+2lka6hmrXh2yt0JaNQgsG7BlZ3OvP6A5Ok1uYqj5ZzSImXj1BIRkcxsFc+OW/kzSq9cRVrH5khu1dixD2krxbzYtcuiL4zU6TW5iqPlntIi5WJGhohIRm7LNDixP5KU6TW5iqOV2O+HlIEZGSIiGbk802AtCyMIkrrzGqfXbGWA5CiOVmK/H1IGZmSIiGTkskzD6tXAPfeI3+eG7QU8XRzt9iXppFoMZIiIZOSSTIMT00iu4MniaCX2+yFl4NQSEZGM6lQ8a20a6eOPZd3k0V2U1u+HlIEZGSIiGTmdaZApCyM3JfT7IWXRCIJ3/60vKyuDVqtFaWkpwsPD5R4OEZEoyZsw+mgAQ75H6uc3MzJERApgN9OQnw+0by/+YAYx5MMYyBAR1VJ7qwBPTV1YLZ5lFobIKgYyREQ1SJ7i8QRrAcyUKcBLL3l2LEQKxUCGiOgGRW1KyCwMkSRcfk1EBPtbBQDVWwXoDW4OJKwtqdbrGcQQiWAgQ0QEBWxKWFxsOwvjx/+uicRwaomICDJvSshpJCKnMcQnIoJMmxKGhIgHMf36MYghkogZGSIiyLApIbMwRC7BjAwREf7aKgCAxb5HLt2U0Foxb3k5gxgiJzCQISK6wZ2bEuovX7GdhWnQwOlrE/kyTi0REdXglk0JNRr4ixxev/+0x/rSyNWtmMjdGMgQEdVidasAR6WlAWvWWBw+HxyOLk+uBDzUZE9R3YqJXIy7XxMRuYOVaaS4yVl/nYLqaavtk+9yW3bEWrdi47N5tFsxkQOkfn6zRoaIyJWsFPP2SF9mFsQA7m+yp5huxURuxECGiMgVBMFmFqY4vInVh7qlyR4U0K2YyANYI0NEVFdWApicY+fw8OKddh/ubJM9ewW8snYrJvIQBjJERM566SVg2jTx+wQB3QyC25rsSSnglaVbMZGHcWqJiMgZGo14ECMIpsZ27mqyZyzgrT1tVFxaifTluVifXwTgr27F1q6uQXXw47JuxUQyYCBDROQIa5159+wR7czr6iZ7jhTweqxbMZGMOLVERCSVk/sjubLJniMFvMmtGpsCqdrTUDr2kSEvwUCGiMgeF2zw6Kome84U8LqlWzGRQjCQISKyZtUq4N57xe+TqZeoswW8LutWTKQwDGSIiMS4IAvjDsYCXneshCJSIxb7EhHVZK2Y98svZQ9iAPethCJSKwYyRERGtrIw993n2bHY4OqVUERqxqklIiKFTiPZwgJeomoMZIjId+3aBfToIX6fm4MYe9sLSMECXiIGMkTkI2oHDsmtrWzi6IEsjJTtBYhIGtbIEJFi6A0CcgpKsDrvFHIKSqA3uCaoWJ9fhN6Zm/Hw4p1I63STeBAzebLHghgp2wsQkTTMyBCRIrgrS2EMHAQAxzMHiZ/koVoYe9sLaFC9vUC/RB1rXYgkkpyROX36tDvHQUQ+zF1ZCmPgUJg5SDSIiZ+0FslzNros82OPI9sLEJE0kgOZW2+9FStXrnTnWIjIBzmyCaKj8nb8gpwXUkTvi5ucBUGj8Wjg4Mz2AkRkm+RAZvbs2Xj88cfxwAMP4Px5flsgItdwW5ZCo0GX2ztaHI6bnIW4yVlmxzwVODi7vQARWSc5kBk7dix++eUXlJSUIDExEWvXrnXnuIjIR7g8S2GlM29+ZCuLAMbIU4GDcXsBa9UvGlTXBXF7ASLpHCr2jY+Px+bNm/HOO+/gvvvuQ0JCAurVM79Ebm6uSwdIRN7NpVkKK43t4idnKWJfIuP2AunLc6EBzMbE7QWInOPwqqXff/8dX331FRo1aoS0tDSLQIaIyBEu2QTRWmfeigqs/60UWC7+BUsA8I+kKI8GDsbtBWqv0NKxjwyRUxyKQhYvXoynn34aKSkpOHDgAJo2bequcRGRj6hTlqKiAggNFb/wjSXVqe1C8K/b47FoW6Hoae9tK0Sn2EaSAwhXdOTl9gJEriM5kElNTcXu3bvxzjvvYMSIEe4cExH5GKeyFBL3R9IbBKzZZ3v5ttTeLa7sdcPtBYhcQ3Igo9fr8csvv+Cmm25y53iIfIorvt17C8lZiltvBQ4eFL+ISGM7R1ZF2QosajbWq8nY64a7ThPJQ3Igs2HDBneOg8jncL8dS3azFE7sUu2KVVHsyEukXNxriUgG3G/HQVaWVOOPP+xuL+CKVVHsyEukXAxkiDzMnZ1slcClGz8aDLazMM2b272EK3q3sCMvkXJx7TSRh7mqZkOJXDpd5sQ0khhX9G5hR14i5WJGhsjDvOXbfe3My7e/nHbNdNm//uWyIMbIuCpKpzUPNHTaIElFuuzIS6RczMgQeZg3fLsXy7z4aVD3YlgXBzA11aV3CzvyEikXMzJEHqb2b/fWCpVtlcLYLYa1Vsybk+OSIMbIuCoqrWNzJLdq7FDgUdesDhG5BzMyRB5Qu1/MlLsTMW6l+r7d2ypUlkJ0usyNWRhXY0deIuVhIEPkZtYKYP91ezzW7CtS1X479gqV7TGbLlNRAFMTO/ISKYusU0vbtm3D4MGDER0dDY1Gg1WrVpndLwgCpk6diqioKAQHByMlJQW//vqrPIMlcoKtfjHvbSvElLsT8MmYHnjroY74ZEwPbJ98l2KDGMD5AmSz6bKFC1UbxBCR8sgayFRUVCApKQnz588XvX/u3LmYN28eFi5ciF27dqFBgwbo378/KiuVvZqDCJDWL2bmN4fQLT7CqZoNOThTgGw2XebvB6SnW54kCD4VxLi01w6Rj5N1amnAgAEYMGCA6H2CIODNN9/Eiy++iLS0NADARx99hMjISKxatQoPPfSQJ4dK5DBv7BdjLFQuLq20WifjpzEv/NVpg5DzQgqQKXLyxx8Dw4a5Y6iKxa0piFxLsTUyhYWFKC4uRkpKiumYVqtF9+7dkZOTYzWQqaqqQlVVlel2WVmZ28dKJMZb+sXUJGUZ8jsPd0ajBgGmYtjk1k3EL+ZDGRgjbjxJ5HqKXX5dXFwMAIiMjDQ7HhkZabpPTEZGBrRareknJibGreMkssYb+sWIsbcMeWCHqOolzp1uEg9iPDSNpLTpG2/fmoJILorNyDjr+eefx8SJE023y8rKGMyQLOxNw2hQ/eGv1H4xtthchrx1K3DnneIP9FAWRonTN9441UikBIrNyOh0OgDAmTNnzI6fOXPGdJ+YwMBAhIeHm/0QycE4DQPAovmd0vvFSCHaXE6jEQ9iPFjMq9Sdxb1xqpFICRQbyMTHx0On02HTpk2mY2VlZdi1axeSk5NlHBmRdD7TDdZaZ96nn/ZoLYySp2+8daqRSG6yTi2Vl5fj2LFjptuFhYXIy8tDREQEYmNj8dRTT2HWrFlo06YN4uPjMWXKFERHR+Oee+6Rb9BEDvL6brAK6gmj5Okbb55qJJKTrIHM3r17cWeNNLSxtmXkyJFYtmwZJk2ahIqKCvzrX//CxYsX0bt3b6xfvx5BQfzGQurild1grQUwBoP1+9xMydM33HiSyD00guDdayDLysqg1WpRWlrKehkiVygoAFq3Fr/PQ/+d1N67ypjhyikowcOLd9p9/CdjesgWWCqxEJlIiaR+fnvdqiUiciMFTCPZCgT6JeoUP33j9VONRB7GjAwR2WctgOnZE9ixw2PDsNZQzji6BcM6AwDSl+cCEJ++8aoiayIvJvXzW7Grlsg3KK1pmaep4vXbysJ4MIiRuiKpX6LON1aKEREATi2RjHy9VkDxr99aAFNZCQQGenYscGxFktKnb6zV+BCR4xjIkCx8fc8ZRb/+sjJAqxW/T8aZaEdXJCl1pZjiA1gileHUEnmckpuWeYKiX79GIx7E2OjM66npMW9oKKfUrsNEasaMDHmckpuWeYIiX3/LlkBhoZUBWQ9MPJldUHtDOXsBrAZ/1fhwmolIOmZkyOOU3LTME1z5+l2SDdFoxIMYO/sjeTq7oPa9qxwJYIlIOmZkyOO8YYqgLlz1+u1lQ+wWlFor5j19GoiynU2RK7tg3Luq9uvWqaDGxNcDeCJ3YSBDHqf2KYK6csXrt1cs/K/b47FmX5F4kJPQDKhn5Z++xGJeOafHUttF4a62kfg45zh+P38ZLSJCMDw5DgH1rCeYlbBKyNcDeCJ3YSBDHufre87U9fVLKRZetM1yqqi4tBKp7aPFB+XgaiQ5swtimaj3txdazcgoZZWQrwfwRO7CGhmShXGKwFebltXl9dvLhoh5Z9XLKMwcJH6nE0uq5couOFqXo6RVQmqv8SFSKmZkSDZKb1rmbs6+fkezHMddGMAYyZFdcLQuR4mrhNRc40OkVAxkSFZKbVrmKc68fqlZDmsBzOARb+CxCQ8gzaFnNSfH9KCjdTmKXOYOBvBErsZAhkhl7GVDAOtBTNzkLACumfLxdHbB0bocJa8S8vUAnsiVGMgQqYytbIi9AMbVUz6ezC44WpfDVUJEvoHFvkQqVLtY+LHdX0kKYgDXT/kYswtpHZsjuVVjt02RGDNR1q6uQfVqJGOQ5uj5RKROzMgQqZQxG+LvL/59JHnORq8qKHW0LsfXl/kT+QqNIMi4na0HlJWVQavVorS0FOHh4XIPh8h1rHXm/eQT4KGHFNEEzh0c7QujlD4yROQYqZ/fDGSI1MhaEOPd/5xNHA3SvDWoI/JmUj+/ObXk5fgfuJexEcDoDQJ2F5T4xO/a0VU/XCVE5L0YyHgxptS9yLffAnffLX6fIPB3TUQ+i1NLXsrapoLG7+e+sA2AIxSdubIzjcTfNRF5I04t+TAltmZXMsVmM6wFMM88A7zyCgD+romI2EfGCznSmt3XKWlTQTO2sjA3ghiAv2siIgYyXkjJrdmVxF42A6jOZugNHpx91WjEgxiDQXRFEn/XROTrGMh4IbZml0ZR2YxDh2xnYazcx981Efk6BjJeiK3ZpVFMNkOjARITLY8Lgt2+MPxdE5GvYyDjhYyt2QFYfMCxNftfZM9mWJtG6tlTcmM7/q6JyNcxkPFStTcVNNJpg7gc9wZZsxm2ppF27HDoUmr6XesNAnIKSrA67xRyCko8W39ERF6JfWS8nKL7oyiAcdUSIL6pYF0CAdH33soGj6iqAgICnHoem88n8ruW6++EYpe5E5Eica+lG3w9kCH73PEBW/ua4ZXl+OWth8RP9uA/QbmCCTbtIyJHMZC5gYEMSeHKbEbtD+3jmYPEn9TD//TkCib0BgG9MzdbXSGmQfU02PbJdzFbSEQm7OxL5AApmwpKyWbU7E3zy5tDEF5VIXotvd4Af5eN3j45OwA7ssydGzsSkaNY7EskgdQOwMYP7eOZg0SDmLjJWYibnOXxTrty9sxRzDJ3IvJKzMgQ2SElm/Hfr/Nx5aoe93aJwXGR8zr9ZwUuhGhNtz39oS1nMCH7Mnci8mrMyBDZISWbcfHSFdzbJUb0/rjJWWZBDOD5D205gwk27SMid2JGhsgOe1kKa8W8cZOzLI4ZC1s9/aFtDCaKSytFM0vuHJexaV/68lxoIL7MnU37iMhZzMgQ2WEtSzFvzVyHgxhAng9tuTsAq6lpHxGpC5dfk2zU0qzPuHy4ZjbDkQDGSAnN3+RuSqeW3zkRyY99ZG5gIKNMcn+gOsq4aqnQSgDz90ffwdGmcRbHn7izFdpEhinqQ5vBBBGpAQOZGxjIKI9qu7xa2R/JVhbmkzE92BuFiMgJbIhHiiRnYzanWQlgOr/0Hc5XXBN/COQp6iUi8jUs9iWPkrMxm8PefdfmLtVz7m0PDeQpniUiomrMyJBH1bUxm8fqO2wEMEbGlTi1a310Cq71ISLyNgxkyKPq0pjNIwXC1gKY9euB/v0tDqe2i0K/RB2LZ4mIZMJAhjzK2cZs1gqEjXsduaRAWEIWRoyUDSeJiMg9WCNDHuVMYzZ7BcJAdYGw3uDkAjyNRjyIEQS7QQwREcmLgQx5nKNdXt1WILxli9NZGCIiUgZOLZEsHKktccvOzQxgiIi8AgMZko212pLaK5OaNAiUdD1JhcTWApgFC4B//1vS8xARkXIwkCFFEVuZpAsPQsOQ+ii9fK1uOzczC0NE5HUYyJBiWFuZdKbsrxVOGsDsfknN5+oQwHBfIiIiZWMgQ4ogZeuChiH1EVjPD8VlVab7bDafO3YMaNNG/AklBDF17Vvj7iCIQRYREQMZUggpK5MuXL6GFY91h59GY//Du47TSHXtW+Pu5n1q2z2ciMhduPyaFEHqiqNz5VVIbtUYaR2bI7lVY8sgxlpPmPR0yUFMXfvWGIOg2oGZMQhan18kaRzWuPv6RERqwkCGFKEuWxeY2MrCvPuu5LHUpW+Nu5v3ub05IBGRyjCQIUUwbl1grcJDg+qpE9GVSdayMNeuObUiqS59a9y9u7eqdg8nIvIABjKkCM5sXYALF2xnYeo5VwJWl+yQW5r3efD6RERqw0CGFMOhrQs0GiBCJDvjgv2R6pIdkhoESW3y5+z1pZ5HRKR2XLVEimJ364LOnYGff7Z84G23Abt3u2QMxuxQ+vJch/vW2Nvd2+jpL/Zh+j/MVxhJWU7t7O7hRETeSiMI3t3WtKysDFqtFqWlpQgPD5d7OFQXHu7M6+wSZ+OqIgBWgxnjKzFmmhx5LmvXr31NIiI1k/r5zUBGodjsrAZrAczFi4BW69andvb3sD6/CNPXHDBr3lebMXsy5e4EjFv5s0XQYyswYR8ZIvJ2DGRuUGMgI8eHlCIDp+vXgfr1xe9TwV/bHcfOYej7u+yeF9GgPs5XXBO9zxjsbJ98l8XvQ5G/MyIiF5H6+c0aGYWpa0dZZ59Tcd/uvWCDx3Pl1rMxNVkLYgDz5dS1dwq3tns4EZEvUfSqpenTp0Oj0Zj9tG3bVu5huY0czc4U1yV2zBivCGIA164c4nJqIiJxis/I3Hrrrdi4caPpdj0ne4OogSPNzpz9Jl5zOqJJaCCmrzlgc6PGGWsPol+iTtKUhSNTHaLn+luJq1UWwBhJWWEU0SAAJRVX7V6Ly6mJiMQpPiqoV68edDqd5POrqqpQVfVXSr+srMwdw3ILdzc7E5tCssWRwMnW9FTt5dQXKqow85tDpnOPZw4Sv2hBAdCypaSxKpGUZdwz09ph5jcHuZyaiMhJig9kfv31V0RHRyMoKAjJycnIyMhAbGys1fMzMjIwY8YMD47QddzZ7Mxa7Y0U9gInW3U9/16ei4Yh9XHxskgdiCDg+NzB4hdVaRamNmOTv9pBnq5GDZKfH5zqWUNERApftbRu3TqUl5fjlltuQVFREWbMmIFTp04hPz8fYWFhoo8Ry8jExMSoYtWS3iCgd+Zmu9/OxVawSLmu1ExMbZ+M6WE1I+Psta1lYZLnbHT49amBvWk3RRZcExHJyCtWLQ0YMMD05w4dOqB79+5o0aIFPv/8c4wePVr0MYGBgQgMdK79u9zq0lHWFnu1N9ZImdZw9NoP/PI9Xlk3T/S+uMlZQB1rgJTK3gojux2NiYhIlKIDmdoaNmyIm2++GceOHZN7KG4jZSrCUc7U1EgNnBy5trUsTNzkLKev6U24nJqIyHGqCmTKy8tRUFCA4cOHyz0Ut3L1t3NnamqkBk5Srm0tgLl32Kv4ubnlcnqu0CEiIqkUHcg888wzGDx4MFq0aIHTp09j2rRp8Pf3x8MPPyz30NzOld/OpW40+Oo/k3CuosqhwMnetaVmYWqOgyt0iIhIKkUHMn/88QcefvhhlJSUoGnTpujduzd27tyJpk2byj00VZFae9OrTROb17FWsCp2bUcCmNrjMAZQbMFPRET2KHrVkiuoca8ld6nLyhh7jzXeH7t/Dz775HnRa1gLYsTGwVU8RES+jZtG3sBAxpwzWQ5rfWIsdme2srVA8pyNFgHJlLsT0ahBgOg4JD8fERF5La9Yfk2u52jtjb39nzQAUttHiz/400+BIUOw3cGtC+w9nyPbJhARkXdjIEM22esTU2hte4EaiT5HgidP7DdFRETeg4GMB6mxeNVaTxer+yPVcabS3ftNERGRd2Eg4yFqLV6t3dMl9kIRtr03RvxkF5RbuXO/KXdRY4BKROQtGMh4gK1NFdOX5yq6eLVmnxhr00im/ZFc/Hxq2A1arQEqEZG38JN7AN7OXvEqUF28qjcoc/GYv58Gm+Y/IhrEvN57KOInZ7l0d2ZjXxrgr1VKRkrbDdoYoNau6TEGqOvzi2QaGRGR72Ag42aOFK8qkkaDkFMnLQ7HTc7CFwMfMcsm6Q0CcgpKsDrvFHIKSpwOzoz7Tem05tNHOm2QYrJXag9QiYi8BaeW3Ey1xatWesLkHD2LsxVX8YlI7xdXTrEofTdorq4iIlIGBjJuVtfiVY8XkpaWAg0bit8nCEgWOeyuGiAl7wat2gCViMjLMJBxs7oUr3q8kNRKFsbWaiRfbWCnxtVVRETeiDUybuZs8apHC0n//W/xIOaxx+wuqVZ9DZCTjAGqtdBMg+qgUymrq4iIvBUDGQ9wtHjVo4WkGg2waJHF4Zxj56Bf9J7dh/vqFIuaVlcREXkzTi15iCPFqx4pJLUyjXTLxC9RVT8QWLxT0jSWL0+xGAPU2tN/OvaRISLyGAYyLmarOFdq8apbsxzXrwP164veFTc5y+y2lGJdtTWwczWlr64iIvJ2DGRcyFXFuW7LcljJwiTP2SiaAZJSrGucYklfngsNYBbM+MoUi5JXVxEReTvWyLiIK4tzXV5I+vbb4kHMgw8i59i5OhfrqqGBHREReSdmZFzA1UuQpWQ5HrotFlm/nLY/lWFnSfXZvFN2xwPYn8biFAsREcmBgYwLuKM411ohacOQ+hAEAW9sPGo6pgsPxPR/3Gqe+bAWwJw9CzRtarrpymksTrEQEZGnMZBxAXcV59bOchw/V4E3Nv5qcV5xWRX+vTwXC4d1RuqtOsDPyoyhSE8YXy/WJSIidWONjAu4cwmyv58G3eIj0KRBIBZt+83muanto8WDGEGw2tiO/VCIiEjNGMi4gDu7vK7PL0LvzM0Y+sEuXL6qFz0n+fdfcDxzkMgdyXY78wIs1iUiIvXi1JIL1GUJsq2+M9Y2Y6xJNIABJAUwNbFYl4iI1IiBjIs40+XVVt+Zfok6qyuhAODY3H+gnmCwOP7eso3418i+Tr0GFusSEZHaMJBxIUeyGtayLca+M0+ltLG6EspaFiZuchZW9Eqq68sgIiJSDQYyLiYlqyGl78zSHcct7rMVwABAo5D66NGSGRUiIvIdLPaVgZS+MxevXDPdvqn0jGgQszW+i9n+SBn3tWdNCxER+RRmZGQgtZ9Mw+D6yJveX/S+mgGMM/s5EREReQMGMjKQ0k/msd1f4cUtSyyO93t0Pn5t2gIAMLpXHFISdV65usjWai4iIiIjBjIysNdN114tjLdnYFy1izgREXk/jSA42HBEZcrKyqDValFaWorw8HCXXNMV2QLjqiXgr74z1gIYvd7gM9kJa6u5jK+WDfqIiHyD1M9vBjIOcmW2wHitS2fPI//NBy1PGDcOeOedug5ZNfQGAb0zN1sthDbu+7R98l1eG8gREVE1qZ/fnFpygL3eL7WzBfYyN6ntoqr3RxLj3fGlKHfsIk5ERN6NgYxEUnq/zFh7EP0SdfD309jP3GzfDvztb5YXO3gQSEhw18tQNHftIk5ERN6LgYxEjmQLSq9ctZm5KXTR/kiuoKTVQe7cRZyIiLwTAxmJpGYBikuvYO53R0QzN4/v/B+ey15meYfBAGg8HzwobXWQvdVcxhoZZ3YRJyIi78TOvhJJzQKcr7hqkbnxN+hxPHOQZRCTmVmdhZEpiElfnmsxVmPWaH1+kcfHZNxFHPhrlZKRvV3EiYjINzGQkciYLbD2EapBdTYjIjTQ7PjWRWNQ8Eqaxfmrf/4DmDTJ9QOVwF69D1Bd76M3eH6qy7iLuE5rHjjqtEFcek1ERBY4tSSRMVuQvjwXGsAsCKiZLdAGBwAAmpZfwJ75wy2u0yN9GYrDm+ATGes8lL46yJFdxImIyLcxkHGAMVtQu65EV6OuRG8Q8P63ryBlf7bZY3/RtcY/Rr5pytzIWeehhtVBUnYRJyIiYiDjIJvZgt9+g3+rVkip9Zj4SWsgaPwUU+fB1UFEROQtGMg4wSJbIAhAWhqwZo3ZeeMfeRmrm7Uz3dYpZL8grg4iIiJvwUCmrjZvBvr2NT82axbw3//idYOAhxRY5yG13kcJYyUiIrKFey05q6oKaN4cKCn561i9esD580BYmOuex42U1keGiIjIiHstudu775oHMWvWAIMHyzceJ3B1EBERqR0DGWfddhsQFQXcfHP19JKfOlvycHUQERGpGQMZZ/XuDZw+LfcoiIiIfJo60whEREREYCBDREREKsZAhoiIiFSLgQwRERGpFgMZIiIiUi0GMkRERKRaXH7tZfQGgQ3uiIjIZzCQ8SKe2HKAgRIRESkJAxkvsT6/COnLcy12sy4urUT68lwsGNa5zsEM92YiIiKlYY2MF9AbBMxYe9AiiAH+2tl6xtqD0Buc3x/UGCjVDGKAvwKl9flFTl+biIjIWQxkvMDuwvMWAUZNAoCi0krsLjzv1PU9ESgRERE5g4GMFzh7yXoQ48x5tbk7UCIiInIWAxkv0CwsyKXn1ebuQImIiMhZDGS8QLf4CERpg2Bt7ZAG1UW53eIjnLq+uwMlIiIiZzGQ8QL+fhpMG5wIABbBjPH2tMGJTi+TdnegRERE5CwGMjLTGwTkFJRgdd4p5BSUOF0wm9ouCguGdYZOa54V0WmD6rz02t2BEhERkbM0giB49VKTsrIyaLValJaWIjw8XO7hmHFHXxZ3NqxjHxkiIvIUqZ/fqghk5s+fj1deeQXFxcVISkrC22+/jW7dukl6rFIDGWsN7Iwhhysa2LkDO/sSEZEnSP38VvzU0meffYaJEydi2rRpyM3NRVJSEvr374+zZ8/KPTSnqbkvi7+fBsmtGiOtY3Mkt2rMIIaIiGSl+EDm9ddfx5gxY/DII48gMTERCxcuREhICJYsWSL30JzGvixERESuoehA5urVq/jpp5+QkpJiOubn54eUlBTk5OSIPqaqqgplZWVmP0rDvixERESuoehA5ty5c9Dr9YiMjDQ7HhkZieLiYtHHZGRkQKvVmn5iYmI8MVSHsC8LERGRayg6kHHG888/j9LSUtPPyZMn5R6SBfZlISIicg1FBzJNmjSBv78/zpw5Y3b8zJkz0Ol0oo8JDAxEeHi42Y/SsC8LERGRayg6kAkICECXLl2wadMm0zGDwYBNmzYhOTlZxpHVnTsb2BEREfmKenIPwJ6JEydi5MiR6Nq1K7p164Y333wTFRUVeOSRR+QeWp2ltotCv0Qd+7IQERE5SfGBzJAhQ/Dnn39i6tSpKC4uRseOHbF+/XqLAmC1MvZlISIiIseporNvXSi1sy8RERFZ5zWdfYmIiIisYSBDREREqsVAhoiIiFSLgQwRERGpFgMZIiIiUi0GMkRERKRaDGSIiIhItRjIEBERkWopvrNvXRn7/ZWVlck8EiIiIpLK+Lltr2+v1wcyly5dAgDExMTIPBIiIiJy1KVLl6DVaq3e7/VbFBgMBpw+fRphYWHQaOq+GWNZWRliYmJw8uRJbnngAXy/PYvvt2fx/fYcvtee5Yr3WxAEXLp0CdHR0fDzs14J4/UZGT8/P9x0000uv254eDj/MXgQ32/P4vvtWXy/PYfvtWfV9f22lYkxYrEvERERqRYDGSIiIlItBjIOCgwMxLRp0xAYGCj3UHwC32/P4vvtWXy/PYfvtWd58v32+mJfIiIi8l7MyBAREZFqMZAhIiIi1WIgQ0RERKrFQIaIiIhUi4GMA+bPn4+4uDgEBQWhe/fu2L17t9xD8koZGRm47bbbEBYWhmbNmuGee+7BkSNH5B6Wz3j55Zeh0Wjw1FNPyT0Ur3Xq1CkMGzYMjRs3RnBwMNq3b4+9e/fKPSyvpNfrMWXKFMTHxyM4OBitWrXCzJkz7e7fQ9Js27YNgwcPRnR0NDQaDVatWmV2vyAImDp1KqKiohAcHIyUlBT8+uuvLh0DAxmJPvvsM0ycOBHTpk1Dbm4ukpKS0L9/f5w9e1buoXmd7OxsjBs3Djt37sSGDRtw7do1/P3vf0dFRYXcQ/N6e/bswaJFi9ChQwe5h+K1Lly4gF69eqF+/fpYt24dDh48iNdeew2NGjWSe2heKTMzEwsWLMA777yDQ4cOITMzE3PnzsXbb78t99C8QkVFBZKSkjB//nzR++fOnYt58+Zh4cKF2LVrFxo0aID+/fujsrLSdYMQSJJu3boJ48aNM93W6/VCdHS0kJGRIeOofMPZs2cFAEJ2drbcQ/Fqly5dEtq0aSNs2LBB6NOnjzB+/Hi5h+SVJk+eLPTu3VvuYfiMu+++W3j00UfNjt13333C0KFDZRqR9wIgfP3116bbBoNB0Ol0wiuvvGI6dvHiRSEwMFD45JNPXPa8zMhIcPXqVfz0009ISUkxHfPz80NKSgpycnJkHJlvKC0tBQBERETIPBLvNm7cONx9991mf8/J9dasWYOuXbvigQceQLNmzdCpUycsXrxY7mF5rZ49e2LTpk04evQoAGDfvn3Yvn07BgwYIPPIvF9hYSGKi4vN/k/RarXo3r27Sz87vX7TSFc4d+4c9Ho9IiMjzY5HRkbi8OHDMo3KNxgMBjz11FPo1asX2rVrJ/dwvNann36K3Nxc7NmzR+6heL3ffvsNCxYswMSJE/HCCy9gz549ePLJJxEQEICRI0fKPTyv89xzz6GsrAxt27aFv78/9Ho9Zs+ejaFDh8o9NK9XXFwMAKKfncb7XIGBDCnauHHjkJ+fj+3bt8s9FK918uRJjB8/Hhs2bEBQUJDcw/F6BoMBXbt2xZw5cwAAnTp1Qn5+PhYuXMhAxg0+//xzrFixAitXrsStt96KvLw8PPXUU4iOjub77SU4tSRBkyZN4O/vjzNnzpgdP3PmDHQ6nUyj8n5PPPEEsrKysGXLFtx0001yD8dr/fTTTzh79iw6d+6MevXqoV69esjOzsa8efNQr1496PV6uYfoVaKiopCYmGh2LCEhASdOnJBpRN7t2WefxXPPPYeHHnoI7du3x/DhwzFhwgRkZGTIPTSvZ/x8dPdnJwMZCQICAtClSxds2rTJdMxgMGDTpk1ITk6WcWTeSRAEPPHEE/j666+xefNmxMfHyz0kr9a3b1/s378feXl5pp+uXbti6NChyMvLg7+/v9xD9Cq9evWyaCdw9OhRtGjRQqYRebfLly/Dz8/8o87f3x8Gg0GmEfmO+Ph46HQ6s8/OsrIy7Nq1y6WfnZxakmjixIkYOXIkunbtim7duuHNN99ERUUFHnnkEbmH5nXGjRuHlStXYvXq1QgLCzPNpWq1WgQHB8s8Ou8TFhZmUX/UoEEDNG7cmHVJbjBhwgT07NkTc+bMwYMPPojdu3fjvffew3vvvSf30LzS4MGDMXv2bMTGxuLWW2/Fzz//jNdffx2PPvqo3EPzCuXl5Th27JjpdmFhIfLy8hAREYHY2Fg89dRTmDVrFtq0aYP4+HhMmTIF0dHRuOeee1w3CJetf/IBb7/9thAbGysEBAQI3bp1E3bu3Cn3kLwSANGfpUuXyj00n8Hl1+61du1aoV27dkJgYKDQtm1b4b333pN7SF6rrKxMGD9+vBAbGysEBQUJLVu2FP773/8KVVVVcg/NK2zZskX0/+uRI0cKglC9BHvKlClCZGSkEBgYKPTt21c4cuSIS8egEQS2NyQiIiJ1Yo0MERERqRYDGSIiIlItBjJERESkWgxkiIiISLUYyBAREZFqMZAhIiIi1WIgQ0RERKrFQIaIiIhUi4EMERERqRYDGSJSFb1ej549e+K+++4zO15aWoqYmBj897//lWlkRCQHblFARKpz9OhRdOzYEYsXL8bQoUMBACNGjMC+ffuwZ88eBAQEyDxCIvIUBjJEpErz5s3D9OnTceDAAezevRsPPPAA9uzZg6SkJLmHRkQexECGiFRJEATcdddd8Pf3x/79+/Gf//wHL774otzDIiIPYyBDRKp1+PBhJCQkoH379sjNzUW9evXkHhIReRiLfYlItZYsWYKQkBAUFhbijz/+kHs4RCQDZmSISJV+/PFH9OnTB99//z1mzZoFANi4cSM0Go3MIyMiT2JGhohU5/Llyxg1ahTS09Nx55134oMPPsDu3buxcOFCuYdGRB7GjAwRqc748ePx7bffYt++fQgJCQEALFq0CM888wz279+PuLg4eQdIRB7DQIaIVCU7Oxt9+/bF1q1b0bt3b7P7+vfvj+vXr3OKiciHMJAhIiIi1WKNDBEREakWAxkiIiJSLQYyREREpFoMZIiIiEi1GMgQERGRajGQISIiItViIENERESqxUCGiIiIVIuBDBEREakWAxkiIiJSLQYyREREpFr/D6iU5cTC9IfYAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neste exemplo:\n",
        "\n",
        "- Criamos dados de treinamento simples usando uma relação linear $y = 2x + 1 + \\epsilon$, onde $\\epsilon$ é ruído.\n",
        "- Definimos uma classe `LinearRegressionModel` que herda de `nn.Module` para encapsular o modelo linear.\n",
        "- Utilizamos o otimizador SGD (Gradiente Descendente Estocástico) e a função de perda Mean Squared Error (MSE).\n",
        "- Treinamos o modelo por várias épocas, atualizando os pesos com base no erro entre as previsões e os rótulos reais.\n",
        "- Visualizamos os resultados, exibindo os dados de treinamento e as previsões do modelo.\n",
        "\n",
        "Este é um exemplo básico de regressão linear com PyTorch. Em aplicações práticas, você geralmente trabalharia com conjuntos de dados mais complexos e modelos mais sofisticados, mas este exemplo serve como uma introdução à implementação de regressão linear utilizando PyTorch."
      ],
      "metadata": {
        "id": "NIunsUUM9org"
      },
      "id": "NIunsUUM9org"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Custo:**\n",
        "O termo \"custo\" em aprendizado de máquina refere-se à medida de quão mal um modelo está performando em seus dados de treinamento. O objetivo durante o treinamento de um modelo é minimizar esse custo. O custo é frequentemente representado por uma função que compara as previsões do modelo com os rótulos reais e atribui uma pontuação que indica o quão distante as previsões estão da verdade.\n",
        "\n",
        "**Função de Custo:**\n",
        "A função de custo, também conhecida como função de perda ou função objetivo, é uma expressão matemática que calcula o custo associado às previsões do modelo em relação aos rótulos reais. A escolha da função de custo depende do tipo de problema que está sendo abordado (classificação, regressão, etc.). Por exemplo, para problemas de regressão linear, a função de custo comum é o erro quadrático médio (Mean Squared Error - MSE), enquanto para problemas de classificação binária, pode ser a entropia cruzada binária.\n",
        "\n",
        "**Gradiente Descendente:**\n",
        "O gradiente descendente é um algoritmo de otimização utilizado para minimizar a função de custo, ajustando iterativamente os parâmetros do modelo. A ideia fundamental é seguir a inclinação negativa (gradiente) da função de custo para encontrar o mínimo. Em cada iteração, os pesos do modelo são ajustados na direção oposta ao gradiente da função de custo em relação aos pesos. Isso é feito multiplicando o gradiente pela taxa de aprendizado e subtraindo esse resultado dos pesos atuais.\n",
        "\n",
        "O processo de treinamento de um modelo com gradiente descendente envolve repetir as seguintes etapas:\n",
        "1. Calcular as previsões do modelo.\n",
        "2. Calcular o valor da função de custo com base nas previsões e nos rótulos reais.\n",
        "3. Calcular o gradiente da função de custo em relação aos parâmetros do modelo (os pesos).\n",
        "4. Atualizar os pesos usando o gradiente descendente.\n",
        "\n",
        "Esse ciclo é repetido até que o modelo atinja um desempenho aceitável ou até que o número predefinido de iterações seja alcançado.\n",
        "\n",
        "Em resumo, o gradiente descendente é uma técnica fundamental para otimizar modelos de aprendizado de máquina, minimizando a função de custo e melhorando o desempenho do modelo nos dados de treinamento."
      ],
      "metadata": {
        "id": "iVRu6I5ZXT80"
      },
      "id": "iVRu6I5ZXT80"
    },
    {
      "cell_type": "markdown",
      "id": "d9e44a74",
      "metadata": {
        "id": "d9e44a74"
      },
      "source": [
        "### Gradiente Descendente Estocástico"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O Gradiente Descendente Estocástico (SGD, do inglês Stochastic Gradient Descent) é uma variação do algoritmo de otimização do Gradiente Descendente, frequentemente usado no treinamento de modelos de aprendizado de máquina e redes neurais. A principal diferença entre o Gradiente Descendente Estocástico e o Gradiente Descendente clássico está na forma como as atualizações dos parâmetros do modelo são realizadas.\n",
        "\n",
        "Aqui está uma visão geral do SGD:\n",
        "\n",
        "1. **Gradiente Descendente Clássico:**\n",
        "   - No Gradiente Descendente clássico, a atualização dos parâmetros é realizada utilizando o gradiente médio dos parâmetros sobre o conjunto de dados de treinamento completo.\n",
        "   - A função de custo é calculada considerando todos os exemplos de treinamento.\n",
        "\n",
        "2. **Gradiente Descendente Estocástico:**\n",
        "   - No SGD, a atualização dos parâmetros é realizada a cada exemplo de treinamento (ou em pequenos lotes, chamados mini lotes).\n",
        "   - Em vez de calcular o gradiente médio, o gradiente é calculado apenas para um exemplo (ou mini lote) aleatório a cada iteração.\n",
        "   - Isso introduz estocasticidade (aleatoriedade) no processo de atualização dos parâmetros.\n",
        "\n",
        "**Vantagens do SGD:**\n",
        "- **Eficiência Computacional:** Utilizar apenas um exemplo ou mini lote por vez é computacionalmente eficiente, especialmente em conjuntos de dados grandes.\n",
        "- **Mais Rápido para Convergência:** A aleatoriedade introduzida pode ajudar a escapar de mínimos locais e acelerar a convergência, especialmente em funções de custo não convexas.\n",
        "\n",
        "**Desafios do SGD:**\n",
        "- **Estocasticidade:** Devido à estocasticidade, o SGD pode levar a uma convergência mais ruidosa. A função de custo pode variar mais ao longo do tempo.\n",
        "- **Taxa de Aprendizado Ajustável:** A escolha apropriada da taxa de aprendizado é crucial. Pode ser necessário ajustar dinamicamente a taxa de aprendizado durante o treinamento.\n",
        "\n",
        "**Exemplo de Uso:**\n",
        "```python\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "# Definindo um modelo e uma função de custo\n",
        "model = torch.nn.Linear(2, 1)\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "# Definindo um otimizador SGD\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Exemplo de treinamento com SGD\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, labels in data_loader:  # Iterando sobre mini lotes\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "```\n",
        "\n",
        "Neste exemplo, `optim.SGD` é usado como otimizador para aplicar o Gradiente Descendente Estocástico durante o treinamento. O loop aninhado sobre mini lotes ilustra o conceito de atualização de parâmetros para cada exemplo ou mini lote."
      ],
      "metadata": {
        "id": "W_m9aN3CslX3"
      },
      "id": "W_m9aN3CslX3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataloader"
      ],
      "metadata": {
        "id": "wYtaheaSukFr"
      },
      "id": "wYtaheaSukFr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "O DataLoader é uma classe em PyTorch que facilita o carregamento eficiente de dados durante o treinamento de modelos de aprendizado de máquina. Ele é parte do módulo `torch.utils.data` e oferece uma interface amigável para trabalhar com conjuntos de dados, especialmente em tarefas de treinamento e validação.\n",
        "\n",
        "Principais funções do DataLoader:\n",
        "\n",
        "1. **Divisão em Mini Lotes:**\n",
        "   - Um dos principais propósitos do DataLoader é dividir automaticamente o conjunto de dados em mini lotes. Cada mini lote contém um subconjunto dos dados, facilitando o treinamento do modelo em partes menores.\n",
        "\n",
        "2. **Embaralhamento e Amostragem Aleatória:**\n",
        "   - O DataLoader pode embaralhar os dados no início de cada época, o que é útil para evitar que o modelo memorize a ordem dos exemplos de treinamento. Além disso, o DataLoader pode amostrar os exemplos de forma aleatória.\n",
        "\n",
        "3. **Paralelismo de Carregamento de Dados:**\n",
        "   - Em casos onde há disponibilidade de recursos, o DataLoader permite carregar mini lotes em paralelo, aproveitando a capacidade do sistema para carregar dados mais rapidamente.\n",
        "\n",
        "4. **Tratamento de Dados Nulos:**\n",
        "   - O DataLoader pode lidar com situações em que há exemplos ausentes ou com rótulos ausentes, facilitando o gerenciamento desses casos durante o treinamento.\n",
        "\n",
        "5. **Preparação de Dados para GPU:**\n",
        "   - Facilita a transferência eficiente dos dados para a GPU, caso ela esteja disponível, permitindo treinamento acelerado.\n",
        "\n",
        "Aqui está um exemplo básico de como usar o DataLoader:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Criando dados fictícios\n",
        "x_train = torch.randn((1000, 3, 64, 64))  # Exemplo de imagens (canais x altura x largura)\n",
        "y_train = torch.randint(0, 2, (1000,))  # Exemplo de rótulos binários\n",
        "\n",
        "# Criando um conjunto de dados PyTorch\n",
        "dataset = TensorDataset(x_train, y_train)\n",
        "\n",
        "# Criando um DataLoader\n",
        "batch_size = 32\n",
        "shuffle = True\n",
        "num_workers = 4  # Número de processos de leitura em paralelo\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
        "\n",
        "# Iterando sobre os mini lotes\n",
        "for inputs, labels in data_loader:\n",
        "    # Treinar o modelo com este mini lote\n",
        "    pass\n",
        "```\n",
        "\n",
        "Neste exemplo, criamos um DataLoader para um conjunto de dados fictício contendo imagens e rótulos. Durante o treinamento do modelo, podemos iterar sobre os mini lotes gerados pelo DataLoader. Esse processo é fundamental para garantir que o modelo tenha acesso eficiente aos dados durante o treinamento."
      ],
      "metadata": {
        "id": "FseEBRMdsqFD"
      },
      "id": "FseEBRMdsqFD"
    },
    {
      "cell_type": "markdown",
      "id": "a88a98ad",
      "metadata": {
        "id": "a88a98ad"
      },
      "source": [
        "### Mini-Batch Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O Mini-Batch Gradient Descent é uma variação do Gradiente Descendente em que, em vez de calcular o gradiente e fazer uma atualização de parâmetros usando o **conjunto de dados completo** (Batch Gradient Descent) ou apenas **um exemplo por vez** (Stochastic Gradient Descent), trabalhamos com mini lotes de dados. Cada atualização de parâmetro é realizada com base em um pequeno subconjunto (mini lote) dos dados de treinamento.\n",
        "\n",
        "Usar o Gradiente Descendente Estocástico (SGD) com o DataLoader em PyTorch é bastante simples. O DataLoader lida com o embaralhamento dos dados e a divisão em mini lotes, enquanto o otimizador SGD é responsável pelas atualizações dos parâmetros do modelo. Aqui está um exemplo básico:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Criando dados fictícios\n",
        "x_train = torch.randn((1000, 3, 64, 64))  # Exemplo de imagens (canais x altura x largura)\n",
        "y_train = torch.randint(0, 2, (1000,))  # Exemplo de rótulos binários\n",
        "\n",
        "# Criando um conjunto de dados PyTorch\n",
        "dataset = TensorDataset(x_train, y_train)\n",
        "\n",
        "# Criando um DataLoader\n",
        "batch_size = 32\n",
        "shuffle = True\n",
        "num_workers = 4  # Número de processos de leitura em paralelo\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
        "\n",
        "# Criando um modelo de exemplo\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.fc = nn.Linear(3 * 64 * 64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)  # Achatando as imagens para um vetor\n",
        "        return self.fc(x)\n",
        "\n",
        "# Criando uma instância do modelo\n",
        "modelo = SimpleModel()\n",
        "\n",
        "# Definindo a função de perda e o otimizador SGD\n",
        "criterio = nn.BCEWithLogitsLoss()  # Para problemas de classificação binária\n",
        "otimizador = optim.SGD(modelo.parameters(), lr=0.01)\n",
        "\n",
        "# Treinando o modelo usando SGD e DataLoader\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, labels in data_loader:\n",
        "        # Forward pass\n",
        "        outputs = modelo(inputs)\n",
        "        loss = criterio(outputs.squeeze(), labels.float())\n",
        "\n",
        "        # Backward pass e otimização\n",
        "        otimizador.zero_grad()\n",
        "        loss.backward()\n",
        "        otimizador.step()\n",
        "\n",
        "    print(f'Época [{epoch + 1}/{num_epochs}], Perda: {loss.item():.4f}')\n",
        "\n",
        "Época [1/10], Perda: 0.9239\n",
        "Época [2/10], Perda: 0.1510\n",
        "Época [3/10], Perda: 0.1002\n",
        "Época [4/10], Perda: 0.0714\n",
        "Época [5/10], Perda: 0.0521\n",
        "Época [6/10], Perda: 0.0513\n",
        "Época [7/10], Perda: 0.0373\n",
        "Época [8/10], Perda: 0.0273\n",
        "Época [9/10], Perda: 0.0294\n",
        "Época [10/10], Perda: 0.0289\n",
        "```\n",
        "\n",
        "Neste exemplo:\n",
        "\n",
        "- Criamos um DataLoader para o conjunto de dados usando a classe `DataLoader`.\n",
        "- Definimos um modelo de exemplo (`SimpleModel`) para fins ilustrativos.\n",
        "- Utilizamos a função de perda `BCEWithLogitsLoss`, que é apropriada para problemas de classificação binária.\n",
        "- Utilizamos o otimizador SGD para ajustar os parâmetros do modelo.\n",
        "- Iteramos sobre os mini lotes gerados pelo DataLoader durante o treinamento, realizando o treinamento por várias épocas.\n",
        "\n",
        "Este é um exemplo básico para demonstrar como usar o DataLoader em conjunto com o SGD para treinar um modelo. Em aplicações práticas, você adaptaria o modelo, a função de perda e outros hiperparâmetros para atender às necessidades específicas do seu problema."
      ],
      "metadata": {
        "id": "U9rq41OQw0yQ"
      },
      "id": "U9rq41OQw0yQ"
    },
    {
      "cell_type": "markdown",
      "id": "98a58c7d",
      "metadata": {
        "id": "98a58c7d"
      },
      "source": [
        "### Treinamento, Validação e Divisão de Dados"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Criando dados fictícios\n",
        "torch.manual_seed(42)\n",
        "X = torch.rand(1000, 1)  # Feature unidimensional\n",
        "y = 2 * X + 1 + 0.1 * torch.rand(1000, 1)  # Regressão linear com ruído\n",
        "\n",
        "# Dividindo os dados em treino, validação e teste\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Convertendo para tensores\n",
        "X_train, y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32)\n",
        "X_val, y_val = torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32)\n",
        "X_test, y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# Criando conjuntos de dados PyTorch\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "val_dataset = TensorDataset(X_val, y_val)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "# Criando DataLoader para treino, validação e teste\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# Definindo um modelo de regressão linear simples\n",
        "class LinearRegressionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LinearRegressionModel, self).__init__()\n",
        "        self.linear = nn.Linear(1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "# Criando uma instância do modelo\n",
        "model = LinearRegressionModel()\n",
        "\n",
        "# Definindo a função de perda e o otimizador\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Treinando o modelo\n",
        "num_epochs = 500\n",
        "train_losses, val_losses = [], []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Modo de treino\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Calculando a perda no conjunto de treino\n",
        "    model.eval()  # Modo de avaliação\n",
        "    with torch.no_grad():\n",
        "        total_val_loss = 0\n",
        "        for inputs_val, labels_val in val_loader:\n",
        "            outputs_val = model(inputs_val)\n",
        "            val_loss = criterion(outputs_val, labels_val)\n",
        "            total_val_loss += val_loss.item()\n",
        "\n",
        "    average_val_loss = total_val_loss / len(val_loader)\n",
        "    val_losses.append(average_val_loss)\n",
        "\n",
        "    print(f'Época [{epoch + 1}/{num_epochs}], Perda de Treino: {loss.item():.4f}, Perda de Validação: {average_val_loss:.4f}')\n",
        "\n",
        "# Testando o modelo\n",
        "model.eval()  # Modo de avaliação\n",
        "with torch.no_grad():\n",
        "    total_test_loss = 0\n",
        "    for inputs_test, labels_test in test_loader:\n",
        "        outputs_test = model(inputs_test)\n",
        "        test_loss = criterion(outputs_test, labels_test)\n",
        "        total_test_loss += test_loss.item()\n",
        "\n",
        "average_test_loss = total_test_loss / len(test_loader)\n",
        "print(f'Perda no Conjunto de Teste: {average_test_loss:.4f}')\n",
        "\n",
        "# Visualizando os resultados\n",
        "plt.scatter(X_test, y_test, label='Dados Reais')\n",
        "plt.plot(X_test, model(X_test).detach().numpy(), color='red', label='Regressão Linear Ajustada')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QYEVzuIQgk8X",
        "outputId": "75af1ebb-7bbb-4c7d-c5a8-3ce73b6de113"
      },
      "id": "QYEVzuIQgk8X",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-5575a6d8be30>:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X_train, y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32)\n",
            "<ipython-input-8-5575a6d8be30>:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X_val, y_val = torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32)\n",
            "<ipython-input-8-5575a6d8be30>:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X_test, y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época [1/500], Perda de Treino: 2.6193, Perda de Validação: 2.7055\n",
            "Época [2/500], Perda de Treino: 1.1110, Perda de Validação: 0.9546\n",
            "Época [3/500], Perda de Treino: 0.3243, Perda de Validação: 0.3721\n",
            "Época [4/500], Perda de Treino: 0.1872, Perda de Validação: 0.1759\n",
            "Época [5/500], Perda de Treino: 0.1071, Perda de Validação: 0.1061\n",
            "Época [6/500], Perda de Treino: 0.0844, Perda de Validação: 0.0796\n",
            "Época [7/500], Perda de Treino: 0.0713, Perda de Validação: 0.0681\n",
            "Época [8/500], Perda de Treino: 0.0509, Perda de Validação: 0.0618\n",
            "Época [9/500], Perda de Treino: 0.0614, Perda de Validação: 0.0574\n",
            "Época [10/500], Perda de Treino: 0.0570, Perda de Validação: 0.0538\n",
            "Época [11/500], Perda de Treino: 0.0333, Perda de Validação: 0.0506\n",
            "Época [12/500], Perda de Treino: 0.0372, Perda de Validação: 0.0477\n",
            "Época [13/500], Perda de Treino: 0.0359, Perda de Validação: 0.0450\n",
            "Época [14/500], Perda de Treino: 0.0448, Perda de Validação: 0.0424\n",
            "Época [15/500], Perda de Treino: 0.0552, Perda de Validação: 0.0400\n",
            "Época [16/500], Perda de Treino: 0.0329, Perda de Validação: 0.0378\n",
            "Época [17/500], Perda de Treino: 0.0442, Perda de Validação: 0.0356\n",
            "Época [18/500], Perda de Treino: 0.0340, Perda de Validação: 0.0336\n",
            "Época [19/500], Perda de Treino: 0.0341, Perda de Validação: 0.0317\n",
            "Época [20/500], Perda de Treino: 0.0296, Perda de Validação: 0.0300\n",
            "Época [21/500], Perda de Treino: 0.0359, Perda de Validação: 0.0283\n",
            "Época [22/500], Perda de Treino: 0.0319, Perda de Validação: 0.0267\n",
            "Época [23/500], Perda de Treino: 0.0258, Perda de Validação: 0.0252\n",
            "Época [24/500], Perda de Treino: 0.0242, Perda de Validação: 0.0238\n",
            "Época [25/500], Perda de Treino: 0.0252, Perda de Validação: 0.0225\n",
            "Época [26/500], Perda de Treino: 0.0179, Perda de Validação: 0.0213\n",
            "Época [27/500], Perda de Treino: 0.0209, Perda de Validação: 0.0201\n",
            "Época [28/500], Perda de Treino: 0.0173, Perda de Validação: 0.0190\n",
            "Época [29/500], Perda de Treino: 0.0239, Perda de Validação: 0.0180\n",
            "Época [30/500], Perda de Treino: 0.0204, Perda de Validação: 0.0170\n",
            "Época [31/500], Perda de Treino: 0.0161, Perda de Validação: 0.0160\n",
            "Época [32/500], Perda de Treino: 0.0130, Perda de Validação: 0.0152\n",
            "Época [33/500], Perda de Treino: 0.0124, Perda de Validação: 0.0144\n",
            "Época [34/500], Perda de Treino: 0.0113, Perda de Validação: 0.0136\n",
            "Época [35/500], Perda de Treino: 0.0161, Perda de Validação: 0.0128\n",
            "Época [36/500], Perda de Treino: 0.0129, Perda de Validação: 0.0122\n",
            "Época [37/500], Perda de Treino: 0.0131, Perda de Validação: 0.0115\n",
            "Época [38/500], Perda de Treino: 0.0118, Perda de Validação: 0.0109\n",
            "Época [39/500], Perda de Treino: 0.0099, Perda de Validação: 0.0103\n",
            "Época [40/500], Perda de Treino: 0.0098, Perda de Validação: 0.0098\n",
            "Época [41/500], Perda de Treino: 0.0088, Perda de Validação: 0.0093\n",
            "Época [42/500], Perda de Treino: 0.0086, Perda de Validação: 0.0088\n",
            "Época [43/500], Perda de Treino: 0.0070, Perda de Validação: 0.0083\n",
            "Época [44/500], Perda de Treino: 0.0061, Perda de Validação: 0.0079\n",
            "Época [45/500], Perda de Treino: 0.0075, Perda de Validação: 0.0075\n",
            "Época [46/500], Perda de Treino: 0.0092, Perda de Validação: 0.0071\n",
            "Época [47/500], Perda de Treino: 0.0055, Perda de Validação: 0.0068\n",
            "Época [48/500], Perda de Treino: 0.0082, Perda de Validação: 0.0064\n",
            "Época [49/500], Perda de Treino: 0.0079, Perda de Validação: 0.0061\n",
            "Época [50/500], Perda de Treino: 0.0055, Perda de Validação: 0.0058\n",
            "Época [51/500], Perda de Treino: 0.0061, Perda de Validação: 0.0055\n",
            "Época [52/500], Perda de Treino: 0.0051, Perda de Validação: 0.0053\n",
            "Época [53/500], Perda de Treino: 0.0037, Perda de Validação: 0.0050\n",
            "Época [54/500], Perda de Treino: 0.0041, Perda de Validação: 0.0048\n",
            "Época [55/500], Perda de Treino: 0.0038, Perda de Validação: 0.0045\n",
            "Época [56/500], Perda de Treino: 0.0037, Perda de Validação: 0.0043\n",
            "Época [57/500], Perda de Treino: 0.0048, Perda de Validação: 0.0041\n",
            "Época [58/500], Perda de Treino: 0.0044, Perda de Validação: 0.0039\n",
            "Época [59/500], Perda de Treino: 0.0051, Perda de Validação: 0.0038\n",
            "Época [60/500], Perda de Treino: 0.0031, Perda de Validação: 0.0036\n",
            "Época [61/500], Perda de Treino: 0.0038, Perda de Validação: 0.0034\n",
            "Época [62/500], Perda de Treino: 0.0045, Perda de Validação: 0.0033\n",
            "Época [63/500], Perda de Treino: 0.0040, Perda de Validação: 0.0031\n",
            "Época [64/500], Perda de Treino: 0.0027, Perda de Validação: 0.0030\n",
            "Época [65/500], Perda de Treino: 0.0040, Perda de Validação: 0.0029\n",
            "Época [66/500], Perda de Treino: 0.0028, Perda de Validação: 0.0028\n",
            "Época [67/500], Perda de Treino: 0.0029, Perda de Validação: 0.0027\n",
            "Época [68/500], Perda de Treino: 0.0019, Perda de Validação: 0.0026\n",
            "Época [69/500], Perda de Treino: 0.0032, Perda de Validação: 0.0025\n",
            "Época [70/500], Perda de Treino: 0.0020, Perda de Validação: 0.0024\n",
            "Época [71/500], Perda de Treino: 0.0023, Perda de Validação: 0.0023\n",
            "Época [72/500], Perda de Treino: 0.0021, Perda de Validação: 0.0022\n",
            "Época [73/500], Perda de Treino: 0.0024, Perda de Validação: 0.0021\n",
            "Época [74/500], Perda de Treino: 0.0016, Perda de Validação: 0.0020\n",
            "Época [75/500], Perda de Treino: 0.0014, Perda de Validação: 0.0020\n",
            "Época [76/500], Perda de Treino: 0.0023, Perda de Validação: 0.0019\n",
            "Época [77/500], Perda de Treino: 0.0012, Perda de Validação: 0.0019\n",
            "Época [78/500], Perda de Treino: 0.0014, Perda de Validação: 0.0018\n",
            "Época [79/500], Perda de Treino: 0.0024, Perda de Validação: 0.0017\n",
            "Época [80/500], Perda de Treino: 0.0021, Perda de Validação: 0.0017\n",
            "Época [81/500], Perda de Treino: 0.0018, Perda de Validação: 0.0016\n",
            "Época [82/500], Perda de Treino: 0.0015, Perda de Validação: 0.0016\n",
            "Época [83/500], Perda de Treino: 0.0016, Perda de Validação: 0.0015\n",
            "Época [84/500], Perda de Treino: 0.0011, Perda de Validação: 0.0015\n",
            "Época [85/500], Perda de Treino: 0.0012, Perda de Validação: 0.0015\n",
            "Época [86/500], Perda de Treino: 0.0015, Perda de Validação: 0.0014\n",
            "Época [87/500], Perda de Treino: 0.0010, Perda de Validação: 0.0014\n",
            "Época [88/500], Perda de Treino: 0.0009, Perda de Validação: 0.0014\n",
            "Época [89/500], Perda de Treino: 0.0012, Perda de Validação: 0.0013\n",
            "Época [90/500], Perda de Treino: 0.0010, Perda de Validação: 0.0013\n",
            "Época [91/500], Perda de Treino: 0.0016, Perda de Validação: 0.0013\n",
            "Época [92/500], Perda de Treino: 0.0013, Perda de Validação: 0.0013\n",
            "Época [93/500], Perda de Treino: 0.0011, Perda de Validação: 0.0012\n",
            "Época [94/500], Perda de Treino: 0.0012, Perda de Validação: 0.0012\n",
            "Época [95/500], Perda de Treino: 0.0011, Perda de Validação: 0.0012\n",
            "Época [96/500], Perda de Treino: 0.0009, Perda de Validação: 0.0012\n",
            "Época [97/500], Perda de Treino: 0.0012, Perda de Validação: 0.0011\n",
            "Época [98/500], Perda de Treino: 0.0019, Perda de Validação: 0.0011\n",
            "Época [99/500], Perda de Treino: 0.0011, Perda de Validação: 0.0011\n",
            "Época [100/500], Perda de Treino: 0.0010, Perda de Validação: 0.0011\n",
            "Época [101/500], Perda de Treino: 0.0009, Perda de Validação: 0.0011\n",
            "Época [102/500], Perda de Treino: 0.0012, Perda de Validação: 0.0011\n",
            "Época [103/500], Perda de Treino: 0.0008, Perda de Validação: 0.0010\n",
            "Época [104/500], Perda de Treino: 0.0010, Perda de Validação: 0.0010\n",
            "Época [105/500], Perda de Treino: 0.0013, Perda de Validação: 0.0010\n",
            "Época [106/500], Perda de Treino: 0.0009, Perda de Validação: 0.0010\n",
            "Época [107/500], Perda de Treino: 0.0012, Perda de Validação: 0.0010\n",
            "Época [108/500], Perda de Treino: 0.0007, Perda de Validação: 0.0010\n",
            "Época [109/500], Perda de Treino: 0.0014, Perda de Validação: 0.0010\n",
            "Época [110/500], Perda de Treino: 0.0009, Perda de Validação: 0.0010\n",
            "Época [111/500], Perda de Treino: 0.0012, Perda de Validação: 0.0010\n",
            "Época [112/500], Perda de Treino: 0.0009, Perda de Validação: 0.0009\n",
            "Época [113/500], Perda de Treino: 0.0014, Perda de Validação: 0.0009\n",
            "Época [114/500], Perda de Treino: 0.0011, Perda de Validação: 0.0009\n",
            "Época [115/500], Perda de Treino: 0.0008, Perda de Validação: 0.0009\n",
            "Época [116/500], Perda de Treino: 0.0008, Perda de Validação: 0.0009\n",
            "Época [117/500], Perda de Treino: 0.0010, Perda de Validação: 0.0009\n",
            "Época [118/500], Perda de Treino: 0.0007, Perda de Validação: 0.0009\n",
            "Época [119/500], Perda de Treino: 0.0011, Perda de Validação: 0.0009\n",
            "Época [120/500], Perda de Treino: 0.0009, Perda de Validação: 0.0009\n",
            "Época [121/500], Perda de Treino: 0.0010, Perda de Validação: 0.0009\n",
            "Época [122/500], Perda de Treino: 0.0008, Perda de Validação: 0.0009\n",
            "Época [123/500], Perda de Treino: 0.0009, Perda de Validação: 0.0009\n",
            "Época [124/500], Perda de Treino: 0.0008, Perda de Validação: 0.0009\n",
            "Época [125/500], Perda de Treino: 0.0009, Perda de Validação: 0.0009\n",
            "Época [126/500], Perda de Treino: 0.0009, Perda de Validação: 0.0009\n",
            "Época [127/500], Perda de Treino: 0.0013, Perda de Validação: 0.0009\n",
            "Época [128/500], Perda de Treino: 0.0009, Perda de Validação: 0.0009\n",
            "Época [129/500], Perda de Treino: 0.0009, Perda de Validação: 0.0009\n",
            "Época [130/500], Perda de Treino: 0.0009, Perda de Validação: 0.0009\n",
            "Época [131/500], Perda de Treino: 0.0008, Perda de Validação: 0.0009\n",
            "Época [132/500], Perda de Treino: 0.0008, Perda de Validação: 0.0009\n",
            "Época [133/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [134/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [135/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [136/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [137/500], Perda de Treino: 0.0010, Perda de Validação: 0.0008\n",
            "Época [138/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [139/500], Perda de Treino: 0.0010, Perda de Validação: 0.0008\n",
            "Época [140/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [141/500], Perda de Treino: 0.0011, Perda de Validação: 0.0008\n",
            "Época [142/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [143/500], Perda de Treino: 0.0010, Perda de Validação: 0.0008\n",
            "Época [144/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [145/500], Perda de Treino: 0.0011, Perda de Validação: 0.0008\n",
            "Época [146/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [147/500], Perda de Treino: 0.0010, Perda de Validação: 0.0008\n",
            "Época [148/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [149/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [150/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [151/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [152/500], Perda de Treino: 0.0006, Perda de Validação: 0.0008\n",
            "Época [153/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [154/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [155/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [156/500], Perda de Treino: 0.0010, Perda de Validação: 0.0008\n",
            "Época [157/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [158/500], Perda de Treino: 0.0011, Perda de Validação: 0.0008\n",
            "Época [159/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [160/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [161/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [162/500], Perda de Treino: 0.0006, Perda de Validação: 0.0008\n",
            "Época [163/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [164/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [165/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [166/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [167/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [168/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [169/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [170/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [171/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [172/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [173/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [174/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [175/500], Perda de Treino: 0.0010, Perda de Validação: 0.0008\n",
            "Época [176/500], Perda de Treino: 0.0006, Perda de Validação: 0.0008\n",
            "Época [177/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [178/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [179/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [180/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [181/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [182/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [183/500], Perda de Treino: 0.0011, Perda de Validação: 0.0008\n",
            "Época [184/500], Perda de Treino: 0.0010, Perda de Validação: 0.0008\n",
            "Época [185/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [186/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [187/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [188/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [189/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [190/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [191/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [192/500], Perda de Treino: 0.0010, Perda de Validação: 0.0008\n",
            "Época [193/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [194/500], Perda de Treino: 0.0010, Perda de Validação: 0.0008\n",
            "Época [195/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [196/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [197/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [198/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [199/500], Perda de Treino: 0.0006, Perda de Validação: 0.0008\n",
            "Época [200/500], Perda de Treino: 0.0011, Perda de Validação: 0.0008\n",
            "Época [201/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [202/500], Perda de Treino: 0.0006, Perda de Validação: 0.0008\n",
            "Época [203/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [204/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [205/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [206/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [207/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [208/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [209/500], Perda de Treino: 0.0005, Perda de Validação: 0.0008\n",
            "Época [210/500], Perda de Treino: 0.0006, Perda de Validação: 0.0008\n",
            "Época [211/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [212/500], Perda de Treino: 0.0010, Perda de Validação: 0.0008\n",
            "Época [213/500], Perda de Treino: 0.0006, Perda de Validação: 0.0008\n",
            "Época [214/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [215/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [216/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [217/500], Perda de Treino: 0.0006, Perda de Validação: 0.0008\n",
            "Época [218/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [219/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [220/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [221/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [222/500], Perda de Treino: 0.0005, Perda de Validação: 0.0008\n",
            "Época [223/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [224/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [225/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [226/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [227/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [228/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [229/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [230/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [231/500], Perda de Treino: 0.0006, Perda de Validação: 0.0008\n",
            "Época [232/500], Perda de Treino: 0.0006, Perda de Validação: 0.0008\n",
            "Época [233/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [234/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [235/500], Perda de Treino: 0.0010, Perda de Validação: 0.0008\n",
            "Época [236/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [237/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [238/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [239/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [240/500], Perda de Treino: 0.0010, Perda de Validação: 0.0008\n",
            "Época [241/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [242/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [243/500], Perda de Treino: 0.0010, Perda de Validação: 0.0008\n",
            "Época [244/500], Perda de Treino: 0.0010, Perda de Validação: 0.0008\n",
            "Época [245/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [246/500], Perda de Treino: 0.0006, Perda de Validação: 0.0008\n",
            "Época [247/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [248/500], Perda de Treino: 0.0010, Perda de Validação: 0.0008\n",
            "Época [249/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [250/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [251/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [252/500], Perda de Treino: 0.0011, Perda de Validação: 0.0008\n",
            "Época [253/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [254/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [255/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [256/500], Perda de Treino: 0.0004, Perda de Validação: 0.0008\n",
            "Época [257/500], Perda de Treino: 0.0010, Perda de Validação: 0.0008\n",
            "Época [258/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [259/500], Perda de Treino: 0.0010, Perda de Validação: 0.0008\n",
            "Época [260/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [261/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [262/500], Perda de Treino: 0.0010, Perda de Validação: 0.0008\n",
            "Época [263/500], Perda de Treino: 0.0011, Perda de Validação: 0.0008\n",
            "Época [264/500], Perda de Treino: 0.0010, Perda de Validação: 0.0008\n",
            "Época [265/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [266/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [267/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [268/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [269/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [270/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [271/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [272/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [273/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [274/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [275/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [276/500], Perda de Treino: 0.0006, Perda de Validação: 0.0008\n",
            "Época [277/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [278/500], Perda de Treino: 0.0006, Perda de Validação: 0.0008\n",
            "Época [279/500], Perda de Treino: 0.0010, Perda de Validação: 0.0008\n",
            "Época [280/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [281/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [282/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [283/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [284/500], Perda de Treino: 0.0010, Perda de Validação: 0.0008\n",
            "Época [285/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [286/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [287/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [288/500], Perda de Treino: 0.0006, Perda de Validação: 0.0008\n",
            "Época [289/500], Perda de Treino: 0.0010, Perda de Validação: 0.0008\n",
            "Época [290/500], Perda de Treino: 0.0010, Perda de Validação: 0.0008\n",
            "Época [291/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [292/500], Perda de Treino: 0.0006, Perda de Validação: 0.0008\n",
            "Época [293/500], Perda de Treino: 0.0010, Perda de Validação: 0.0008\n",
            "Época [294/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [295/500], Perda de Treino: 0.0011, Perda de Validação: 0.0008\n",
            "Época [296/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [297/500], Perda de Treino: 0.0010, Perda de Validação: 0.0008\n",
            "Época [298/500], Perda de Treino: 0.0011, Perda de Validação: 0.0008\n",
            "Época [299/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [300/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [301/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [302/500], Perda de Treino: 0.0010, Perda de Validação: 0.0008\n",
            "Época [303/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [304/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [305/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [306/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [307/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [308/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [309/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [310/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [311/500], Perda de Treino: 0.0006, Perda de Validação: 0.0008\n",
            "Época [312/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [313/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [314/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [315/500], Perda de Treino: 0.0006, Perda de Validação: 0.0008\n",
            "Época [316/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [317/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [318/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [319/500], Perda de Treino: 0.0011, Perda de Validação: 0.0008\n",
            "Época [320/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [321/500], Perda de Treino: 0.0010, Perda de Validação: 0.0008\n",
            "Época [322/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [323/500], Perda de Treino: 0.0006, Perda de Validação: 0.0008\n",
            "Época [324/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [325/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [326/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [327/500], Perda de Treino: 0.0006, Perda de Validação: 0.0008\n",
            "Época [328/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [329/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [330/500], Perda de Treino: 0.0005, Perda de Validação: 0.0008\n",
            "Época [331/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [332/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [333/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [334/500], Perda de Treino: 0.0010, Perda de Validação: 0.0008\n",
            "Época [335/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [336/500], Perda de Treino: 0.0011, Perda de Validação: 0.0008\n",
            "Época [337/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [338/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [339/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [340/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [341/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [342/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [343/500], Perda de Treino: 0.0006, Perda de Validação: 0.0008\n",
            "Época [344/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [345/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [346/500], Perda de Treino: 0.0010, Perda de Validação: 0.0008\n",
            "Época [347/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [348/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [349/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [350/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [351/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [352/500], Perda de Treino: 0.0010, Perda de Validação: 0.0008\n",
            "Época [353/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [354/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [355/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [356/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [357/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [358/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [359/500], Perda de Treino: 0.0006, Perda de Validação: 0.0008\n",
            "Época [360/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [361/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [362/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [363/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [364/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [365/500], Perda de Treino: 0.0011, Perda de Validação: 0.0008\n",
            "Época [366/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [367/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [368/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [369/500], Perda de Treino: 0.0010, Perda de Validação: 0.0008\n",
            "Época [370/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [371/500], Perda de Treino: 0.0006, Perda de Validação: 0.0008\n",
            "Época [372/500], Perda de Treino: 0.0006, Perda de Validação: 0.0008\n",
            "Época [373/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [374/500], Perda de Treino: 0.0006, Perda de Validação: 0.0008\n",
            "Época [375/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [376/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [377/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [378/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [379/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [380/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [381/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [382/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [383/500], Perda de Treino: 0.0010, Perda de Validação: 0.0008\n",
            "Época [384/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [385/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [386/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [387/500], Perda de Treino: 0.0006, Perda de Validação: 0.0008\n",
            "Época [388/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [389/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [390/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [391/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [392/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [393/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [394/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [395/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [396/500], Perda de Treino: 0.0010, Perda de Validação: 0.0008\n",
            "Época [397/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [398/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [399/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [400/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [401/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [402/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [403/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [404/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [405/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [406/500], Perda de Treino: 0.0005, Perda de Validação: 0.0008\n",
            "Época [407/500], Perda de Treino: 0.0010, Perda de Validação: 0.0008\n",
            "Época [408/500], Perda de Treino: 0.0006, Perda de Validação: 0.0008\n",
            "Época [409/500], Perda de Treino: 0.0010, Perda de Validação: 0.0008\n",
            "Época [410/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [411/500], Perda de Treino: 0.0006, Perda de Validação: 0.0008\n",
            "Época [412/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [413/500], Perda de Treino: 0.0010, Perda de Validação: 0.0008\n",
            "Época [414/500], Perda de Treino: 0.0010, Perda de Validação: 0.0008\n",
            "Época [415/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [416/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [417/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [418/500], Perda de Treino: 0.0006, Perda de Validação: 0.0008\n",
            "Época [419/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [420/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [421/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [422/500], Perda de Treino: 0.0011, Perda de Validação: 0.0008\n",
            "Época [423/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [424/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [425/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [426/500], Perda de Treino: 0.0010, Perda de Validação: 0.0008\n",
            "Época [427/500], Perda de Treino: 0.0010, Perda de Validação: 0.0008\n",
            "Época [428/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [429/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [430/500], Perda de Treino: 0.0010, Perda de Validação: 0.0008\n",
            "Época [431/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [432/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [433/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [434/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [435/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [436/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [437/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [438/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [439/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [440/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [441/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [442/500], Perda de Treino: 0.0010, Perda de Validação: 0.0008\n",
            "Época [443/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [444/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [445/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [446/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [447/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [448/500], Perda de Treino: 0.0006, Perda de Validação: 0.0008\n",
            "Época [449/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [450/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [451/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [452/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [453/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [454/500], Perda de Treino: 0.0010, Perda de Validação: 0.0008\n",
            "Época [455/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [456/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [457/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [458/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [459/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [460/500], Perda de Treino: 0.0005, Perda de Validação: 0.0008\n",
            "Época [461/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [462/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [463/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [464/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [465/500], Perda de Treino: 0.0006, Perda de Validação: 0.0008\n",
            "Época [466/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [467/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [468/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [469/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [470/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [471/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [472/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [473/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [474/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [475/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [476/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [477/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [478/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [479/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [480/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [481/500], Perda de Treino: 0.0006, Perda de Validação: 0.0008\n",
            "Época [482/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [483/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [484/500], Perda de Treino: 0.0010, Perda de Validação: 0.0008\n",
            "Época [485/500], Perda de Treino: 0.0006, Perda de Validação: 0.0008\n",
            "Época [486/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [487/500], Perda de Treino: 0.0010, Perda de Validação: 0.0008\n",
            "Época [488/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [489/500], Perda de Treino: 0.0006, Perda de Validação: 0.0008\n",
            "Época [490/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [491/500], Perda de Treino: 0.0008, Perda de Validação: 0.0008\n",
            "Época [492/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [493/500], Perda de Treino: 0.0006, Perda de Validação: 0.0008\n",
            "Época [494/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [495/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [496/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [497/500], Perda de Treino: 0.0009, Perda de Validação: 0.0008\n",
            "Época [498/500], Perda de Treino: 0.0006, Perda de Validação: 0.0008\n",
            "Época [499/500], Perda de Treino: 0.0007, Perda de Validação: 0.0008\n",
            "Época [500/500], Perda de Treino: 0.0006, Perda de Validação: 0.0008\n",
            "Perda no Conjunto de Teste: 0.0009\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este exemplo:\n",
        "\n",
        "- Gera dados fictícios para uma regressão linear.\n",
        "- Divide os dados em conjuntos de treino, validação e teste.\n",
        "- Cria DataLoader para cada conjunto de dados.\n",
        "- Define um modelo de regressão linear.\n",
        "- Utiliza o Gradiente Descendente Estocástico para treinar o modelo.\n",
        "- Avalia o desempenho do modelo no conjunto de validação.\n",
        "- Testa o modelo no conjunto de teste.\n",
        "- Visualiza os resultados.\n",
        "\n",
        "Os hiperparâmetros podem ser ajustados, como o número de épocas, a taxa de aprendizado, etc., de acordo com as necessidades do seu problema."
      ],
      "metadata": {
        "id": "WgchhH8Agm__"
      },
      "id": "WgchhH8Agm__"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Semana 3"
      ],
      "metadata": {
        "id": "6gE8DljKVS7a"
      },
      "id": "6gE8DljKVS7a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regressão Linear Multipla"
      ],
      "metadata": {
        "id": "WsrL2LHDVS3R"
      },
      "id": "WsrL2LHDVS3R"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regressão Linear de Múltiplas Saídas"
      ],
      "metadata": {
        "id": "89joTJa4VSt3"
      },
      "id": "89joTJa4VSt3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regressão Logística para Classificação"
      ],
      "metadata": {
        "id": "ml8DtfXCVSTP"
      },
      "id": "ml8DtfXCVSTP"
    },
    {
      "cell_type": "markdown",
      "id": "8008dfb8",
      "metadata": {
        "id": "8008dfb8"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Semana 4\n",
        "### Previsão Softmax\n",
        "### Função Softmax\n",
        "### Softmax Pytorch\n",
        "### Redes Neurais Rasas\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Semana 5"
      ],
      "metadata": {
        "id": "SX2oQyuphZbk"
      },
      "id": "SX2oQyuphZbk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Redes Neurais Profundas"
      ],
      "metadata": {
        "id": "A0Ko1HPlhbty"
      },
      "id": "A0Ko1HPlhbty"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Desistência"
      ],
      "metadata": {
        "id": "tVOU-0ZXheco"
      },
      "id": "tVOU-0ZXheco"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pesos e Inicialização da Rede Neural"
      ],
      "metadata": {
        "id": "ZYWmi7P4hf86"
      },
      "id": "ZYWmi7P4hf86"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient Descent com Momentum"
      ],
      "metadata": {
        "id": "Pjii9g4qhh-u"
      },
      "id": "Pjii9g4qhh-u"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalização em Lote"
      ],
      "metadata": {
        "id": "h8olddwihjXd"
      },
      "id": "h8olddwihjXd"
    },
    {
      "cell_type": "markdown",
      "id": "9cd231f3",
      "metadata": {
        "id": "9cd231f3"
      },
      "source": [
        "## Semana 6"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b1a92ea",
      "metadata": {
        "id": "0b1a92ea"
      },
      "source": [
        "### Convolução"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5704020a",
      "metadata": {
        "id": "5704020a"
      },
      "source": [
        "### Funções de Ativação e Max Polling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a358cb5c",
      "metadata": {
        "id": "a358cb5c"
      },
      "source": [
        "### Vários Canais de Entrada e Saída"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "J8FhZKVchX9p"
      },
      "id": "J8FhZKVchX9p"
    },
    {
      "cell_type": "markdown",
      "id": "ed65ebd0",
      "metadata": {
        "id": "ed65ebd0"
      },
      "source": [
        "### Rede Neural Convolucional"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f55c553",
      "metadata": {
        "id": "4f55c553"
      },
      "source": [
        "### Modelos de Visão de Lanterna"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12cc4f30",
      "metadata": {
        "id": "12cc4f30"
      },
      "source": [
        "## Modelos de Visão de Lanterna"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "319.238px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}